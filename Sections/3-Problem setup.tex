\newpage
%\section{Problem Setup}
\chapter{Model formulation}
\label{Chapter_Problem_setup}
%Describe the box task, both versions. 
The box task is an information sampling task that is used to assess a 'jumping to conclusions' (JTC) bias \citep{balzan2017}. In the box task used in this report the participants are shown a grid of twelve boxes, and each time a box is opened, one out of two colours, for example blue or red, is displayed. The participants are told that one of the colours is always in majority, and that their task is to find out which one. We are using two different versions of the box task. In the first one the participant can open as many of the twelve boxes as they want to before deciding which of the two colours that is in majority. We call this the unlimited version. In the second one, which we call the limited version, the participants are told that the test will terminate at one point when a random box is opened. If the participant already have decided on what the majority colour is, this does not influence anything, but if they have not decided yet when the test terminates, this counts as a failed trial. 

We have data from 76 participants that have done multiple trials of both versions of the box task. The experiment where the data was collected was carried out by Professor Gerit Pfuhl and Doctoral Research Fellow Kristoffer Klevjer at UiT The Arctic University of Norway in February 2020. The participants were recruited from an undergraduate psychology course. They did a practice limited trial first that terminated after three boxes where opened, such that if they tried to open a fourth box, the test terminated before they could make a decision. That trial is not analyzed here. Following the practice trial where three unlimited trials. The participants could in these trials open as many of the twelve boxes as they wanted before deciding on what they think is the dominant colour. Lastly, there where six limited trials. Three of them terminated after six boxes had been opened and the other three terminated after nine boxes had been opened. We have data for how many boxes each participant opened before they either chose what they believed was the majority colour, or the test terminated, we call this 'draws to decision'. In addition to this we have information about the choice they made for what the majority colour was, or weather the test terminated before they were able to choose. To compensate for possible biases towards one colour, the two colours where changed for each new trial. They could for example be green and pink in the first trial and blue and yellow in the second trial. For simplicity, we are referring to these colour as blue and red for all trials in this report.

For each trial there is a fixed sequence of boxes. The participant are only able to choose whether to open the next box or not, not which box that opens. Thus, we know how many of the boxes that were blue and how many that were red for each step in the trials. 



\begin{figure}
    \centering
    \includegraphics[scale=0.8]{pictures/dtd2_histogram.png}
    \caption[Draws to decisions trial 2]{Histogram of the draws to decisions for all participants in trial 2.}
    \label{fig:histogram_trial2}
\end{figure}

for example: unlimited
we know trial 2 is ... (1,1,1,0,...)
and trial 3 is (0,1,0,1,0,..). can clearly see the difference in when the participants chose. 

exmaple limited:
trial 5 
trial 8, many more that has the test terminating than in trial 5
show histograms. 



how the different trials look. the arrays with 0 and 1's. 
plots of how many boxes were opened in the different trials. 




About what we will do in this chapter. 

In this chapter, we formulate a model for how decisions are made in the box task, and we want to find parameter estimates such that we can adjust the model to each person. We also find a so-called Ideal Observer solution of the box task. An Ideal Observer would always make optimal decisions \citep{idealObs}. Thus, an Ideal Observer solution is close to an optimal solution of the box task. 



\section{Modelling Framework}
Before we start with the formulation of the model, we will introduce some notation and present some assumptions. 

\subsection{Modelling Assumptions}
\label{section_notation}
Let $X_i$ be the colour of the $i$-th opened box. If the box is blue, $X_i$ is 0 and if the box is red, $X_i$ is one. Thus,
\begin{equation*}
    X_i = \begin{cases}
    0,& \quad \text{if box } i \text{ is blue,}\\
    1,& \quad \text{if box } i \text{ is red.}
    \end{cases}
\end{equation*}
We assume that each $X_i$ has a Bernoulli distribution with success probability $\theta$, such that
\begin{equation*}
    X_i \sim \text{Bernoulli}(\theta).
\end{equation*}
We define a vector, $X_{1:i}$, that, when $i$ boxes are opened,
contains the colours of all these $i$ boxes, such that $X_{1:i} = (X_1,X_2,...,X_{i})$. In the same way, when we have observed data, $(X_1=x_1,X_2=x_2,...,X_i=x_i)$, we denote this as $x_{1:i}$.
%Maybe denote $\textbf{X} = (X_1,X_2,...,X_{12})$ or $\textbf{X}_{12} = (X_1,X_2,...,X_{12})$ and $x_{1:i} = (X_1,X_2,...,X_{i})$?

Additionally, let $U_i$ be the number of the first $i$ opened boxes that are red. Thus, $U_i$ is a stochastic variable defined as
\begin{equation*}
    U_i = \sum_{j=1}^{i} X_j. 
\end{equation*}
The sum of Bernoulli distributed variables is binomial distributed \citep{statinf}. Thus, $U_i$ is binomially distributed with parameters $i$ and $\theta$. We define another stochastic variable, $V_i$, that is the number of red boxes that are not opened when $i$ boxes are opened. Thus, $V_i$ is the number of red boxes out of the $12-i$ boxes that are not opened, which yields,
\begin{equation*}
    V_i = \sum_{j=i+1}^{12} X_j.
\end{equation*}
This variable is also binomially distributed, but with parameters $12-i$ and $\theta$. Thus,
\begin{equation}
\label{U_V_binomal_distri}
    \begin{aligned}
        U_i &\sim \text{Binomial}(i,\theta)\\
        V_i &\sim \text{Binomial}(12-i,\theta).
    \end{aligned}
\end{equation}
%\begin{subequations}
%\begin{equation}
%\label{U_binomail_distr}
%    U_i \sim \text{Binomial}(i,\theta)
%\end{equation}
%\begin{equation}
%\label{V_binomial_distr}
%    V_i \sim \text{Binomial}(12-i,\theta).
%\end{equation}
%\end{subequations}
Then, we have that 
\begin{equation}
\label{ui_prob_mass}
    P(U_i=u_i|\Theta=\theta) = \binom{12}{u_i} \theta^{u_i}(1-\theta)^{12-u_i},
\end{equation}
and
\begin{equation}
\label{vi_prob_mass}
    P(V_i=v_i|\Theta=\theta) = \binom{12-i}{v_i} \theta^{v_i}(1-\theta)^{12-i-v_i},
\end{equation}

Just as in Section \ref{theory_bayesian_modelling}, we let $\Theta$ have a beta prior with parameters $\gamma$ and $\kappa$. (burde jeg skrive om hvorfor her? eller holder det at jeg sier at de er konjugert i 2.4?)
Thus, 
\begin{equation}
\label{beta_prior_theta}
    \Theta \sim \text{Beta}(\gamma,\kappa).
\end{equation}
Hence,
\begin{equation}
\label{beta_density_func}
    f(\theta) = \frac{1}{\text{B}(\gamma,\kappa)}\theta^{\gamma-1}(1-\theta)^{\kappa-1}.
\end{equation}

figure: 
orange: gamma and kappa =1
purple: gamma and kappa =0.5 
pink: 0.1
black: gamma and kappa = 2
grey: gamma=gamma=5 or 10


Figure .. shows the beta distribution (or teh pdf of teh beta dsitr?) for different values of $\gamma$ and $\kappa$. The orange line represents the situation where $\gamma=\kappa=1$. This is the same as a uniform prior for $\Theta$. That means that we have the same probability of $\Theta$ being any value between 0 and 1. As the participant only is informed about one of the colours being in majority, this might be a suitable prior.
However, one might argue that our prior beliefs resembles more the purple or pink lines as we know that one of the colours will definitively be in majority, thus it is not really reasonable to assume that $\theta$ is 0.5. For this reason we also exclude all priors that have $\gamma$ and $\kappa$ larger than 1, which is the situation for the black and grey lines. (Litt usikker på om dette skal være med her, eller lenger nede når jeg snakker om resultatene?)



Of all the 12 boxes, $U_i+V_i$ is the total number of red boxes.
Consequently, if $U_i+V_i$ is bigger than 6, it is a red majority in the box task, and if it is smaller than 6, the true majority colour is blue. We denote this true majority colour as $Z$, such that
\begin{equation}
\label{def_of_Z_2}
    Z = I(U_i+V_i>6).
\end{equation}
This is the same as defining $Z$ as in \eqref{def_of_Z}, as $U_i+V_i = \sum_{j=1}^{12}X_j$, and the order that the boxes are opened in does not affect the majority. Then, as in \eqref{Z_true_majority}, $Z$ is 0 if the true majority colour is blue and 1 if the true majority colour is red. 


%Each time a box is opened, we have three choices. The first is to choose that blue is the majority colour, the second that red is and the third is to open another box. Let $\delta_{i+1}$ denote the different choices when $i$ boxes are opened. If $\delta_{i+1}=0$, we have chosen that blue is the majority colour, $\delta_{i+1}=1$ shows that red is chosen as the majority and $\delta_{i+1}=2$ represents the choice of opening the next box, which is box $i+1$. Moreover, let $\delta_{(i+2):12}$ be the choices made after box $i+1$ is opened. 

Each time a box is opened, the participants have three choices. The first is to choose that blue is the dominant colour, the second that red is, and the third is to choose to open another box. We denote these decisions as $\delta_i$, where $i$ is the number of boxes that are opened. If $\delta_i = 0$, the participant chooses that blue is the more prominent colour, thus that there are in total, of all twelve boxes, more blue boxes than red. Moreover, $\delta_i=1$ means that the participant has chosen that red is the dominant colour, and $\delta_i=2$ represents the situation where the participant chooses to open the next box. Thus,
\begin{equation}
\label{def_of_delta}
    \delta_i =
    \begin{cases}
        0,& \quad \text{if blue is chosen as majority colour,}\\
        1,& \quad \text{if red is chosen as majority colour,}\\
        2,& \quad \text{if the participant chooses to open the next box.}
    \end{cases}
\end{equation}
%We denote the choices that are made from when $i$ boxes are opened to the last choice is made as $\delta_{i:12}$. Something about not making decisions after majority colour is chosen, such that for example $\delta_{12}$ might not exist. dette trgner jeg ikke hvis jeg isf bruker IO(\textbf{x}$_i,\delta_{i+1}$).

We can define loss functions for each of these decisions. These loss functions depends on the true majority colour, $Z$, and the decision that is made, $\delta_i$. Similarly to Chapter \ref{theory_loss_functions}, we denote the loss function when $i$ boxes are opened as $L_i[Z,\delta_i]$. Then, $\Omega_Z = \{0,1\}$ and $\Omega_{\delta_i}=\{0,1,2\}$.


If we take the expectation of this loss function, we get the expected loss for each of these decisions when $i$ boxes are opened, which we denote $\EE^i_{\delta_i}(\theta)$, as it depends on some parameters $\theta$. 

(skal dette avsnittet være her?)
In the limited version of the box task, the participants are told that the test will terminate when a random box is opened. Thus, we need a random variable that represents how many boxes that are open when the test terminates. We call this variable $T$. If $T=3$, then the participant has opened three boxes and wants to open the fourth. Instead of seeing the colour of the fourth box, the test terminates. The information given to the participants regarding this is that the test will terminate when a random box is opened. We assume that the first box always can be opened, but that the probabilities that the test terminates at the subsequent boxes are the same. When 12 boxes are opened, there are no more boxes to open, thus, no more chances for the test to terminate. Thus, $T$ is uniformly distributed with values $(1,2,3,4,5,6,7,8,9,10,11)$, 
\begin{equation}
\label{T_uniform}
    T \sim \text{Uniform}({1,2,3,4,5,6,7,8,9,10,11}).
\end{equation}

Now we have all the notation needed to define the model for how the participants make decisions. 

\subsection{The Model for Decisions/something else}
Having the notation for the expected losses and decisions, we can define the probability mass function for the decisions using a softmax function similar to the one in \eqref{softmax_in_theory}. Thus, the probability mass function for the decisions can be expressed as
\begin{equation}
\label{softmax_real}
    f(\delta_{i}|\theta,\eta) = \frac{\text{exp}(- \eta \EE^i_{\delta_{i}}(\theta))}{\sum_{d=0}^{2} \text{exp}(-\eta \EE^i_{d}(\theta))},
\end{equation}
where $\eta$ is some parameter. This can be interpreted as a measure of how far the choices the participants make are away from the decision with the least expected loss. If $\eta$ is infinity, they always make the decision with the lowest expected loss, and if $\eta$ is zero, they choose arbitrarily. A negative value of $\eta$ indicates that the participant tends to choose the decisions with the highest expected losses.


When we have this model, we can find estimates of the parameters $\theta$ and $\eta$ for each participant such that the model is adapted to each one of the participants. This is done by finding the maximum likelihood estimates (MLEs) as described in Chapter \ref{section_theory_mle}. 
We can also find confidence intervals tied to each of the parameters for all of the participants using the bootstrap as described in Chapter \ref{section_theory_bootstrap}. This will be done in the subsequent sections, but firstly we find an Ideal Observer solution of the box task, and find expressions for the loss functions and expected losses. 


\section{An Ideal Observer Solution of the Box Task}
\sectionmark{An Ideal Observer Solution}
Now we want to find an Ideal Observer solution of the box task. As stated above, an Ideal Observer acts like a participants that always makes optimal decisions. In the case of the box task, the optimal decision for each opened box is the decision that gives the least expected loss. If one makes the decision with the lowest expected loss each time a box is opened, the total solution is the Ideal Observer solution. Thus, we need to find these expected losses. 

Recall that when $i$ boxes are opened, the participants has three choices as stated in \eqref{def_of_delta}. We want to find expected losses for all of these three choices, in both the unlimited and limited versions of the box task. As the expected losses are the expectations of loss functions, we firstly need to define some loss functions. 



\subsection{Loss Functions in the Unlimited Case}
Starting with the unlimited case, we find loss functions for each of the three choices we have when $i$ boxes are opened. If the participant chooses blue as the majority colour, when $\delta_i=0$, we say that the loss is 0 if blue actually is the true majority colour and 1 if it is not. Thus, this can be expressed as an indicator function as in \eqref{loss_func_indicator}. Recall that the true majority colour is denoted $Z$. Then, we can express the loss of choosing blue as the majority colour when $i$ boxes are opened as
\begin{equation}
\label{loss_func_blue}
    L_i[Z,\delta_i=0] = I(Z \neq \delta_i) = I(Z\neq0) = I(Z=1).
\end{equation}

We define the loss function for when the participant chooses that red is the majority colour, when $\delta_i=1$, similarly to \eqref{loss_func_blue}. This time the loss is 0 if the true majority colour is red and 1 if the blue is the true majority colour. Thus,
\begin{equation}
\label{loss_func_red}
    L_i[Z,\delta_i=1] = I(Z\neq\delta_i) = I(Z\neq1)=I(Z=0).
\end{equation}
Putting these two loss functions together, we get that the loss when the participant chooses what the majority colour is, when $\delta_i \neq 2$, is 
\begin{equation}
\label{loss_0_1_unlim}
    L_i[Z,\delta_i \neq 2] = I(Z \neq \delta_i). 
\end{equation}

We imagine that some participants have some small penalty or loss of opening another box. This might be because it is tiresome for them to sit through a whole trial and they want to finish fast, or that they get some kind of inner reward or feeling of victory when they finish early. This is represented by a parameter, $\alpha$. (burde jeg egntlig beskrive hva $\alpha$ er i Modelling Assumptions kapittelet?)
The loss function for the choice of opening the next box depends on the successive(?) losses. As we do not know the choices that will be done later, we do not know what these losses are either. However, we can model these choices as the choices that an Ideal Observer would make. These choices depends on the colour of the next box, $X_{i+1}$ and the colours of the boxes that already are opened, $x_{1:i}$. We denote these choices as $IO(x_{1:i},X_{i+1})$, where $X_{i+1} \in \{0,1\}$ (burde jeg ha liten x for de boksene som allerede er åpnet og stor X for boks i+1 siden det er en sv som vi ikke vet hva er enda?)
%The loss function for opening the next box depends on the parameter $\alpha$. Recall that this is a small loss, potentially zero, for opening a box. It also depends on the loss after the next box is opened. 
We define the loss for the decision to open the next box as $\alpha$ plus the loss in the next step. Thus, 
\begin{equation}
\label{loss_2_unlim}
    L_i[Z,\delta_i=2] = \alpha + L_{i+1}[Z,IO(x_{1:i},X_{i+1})],
\end{equation}
where $L_{i+1}[Z,IO(x_{1:i},X_{i+1})]$ is the loss when the next box is opened. 


Putting \eqref{loss_0_1_unlim} and \eqref{loss_2_unlim} together, we get that the total loss function in the unlimited case can be expressed as
\begin{equation*}
%\label{tot_loss_unlim}
    \begin{aligned}
       L_i[Z,\delta_{i}] 
       = I(Z \neq \delta_i)I(\delta_i\neq2)
       + \big(\alpha + L_{i+1}[Z,IO(x_{1:i},X_{i+1})] \big) I(\delta_i=2).
    \end{aligned}
\end{equation*}

Having found the loss functions for the unlimited case, we proceed with finding the loss functions for the limited trials of the box task. 


\subsection{Loss Functions in the Limited Case}
The loss functions in the limited case are highly comparable to the ones in the unlimited case. Recall that in a limited trial, the participants might be stopped when a random box is opened, and that this counts as a failed trial. 

Firstly, we have a look at the loss function for choosing blue as the majority colour. We see that in the unlimited case, this is not dependent on any of the boxes that are not opened. When $i$ boxes are opened in a limited trial, and the participant chooses that blue is the majority colour, this is, as in the unlimited trial, not affected by the colours of the unopened boxes. If $i$ boxes are opened and one chooses what the majority colour is here, we know that the test will not terminate, as the participant will not open more boxes. Thus, we can put the loss function for choosing blue as the majority colour in a limited trial as the same as the loss function for choosing blue in an unlimited trial. Thus the loss function is as in \eqref{loss_func_blue}.

The same argument holds for the loss function for choosing red as the majority colour in a limited trial. Thus, that loss function is the same as in \eqref{loss_func_red}.

For the choice of opening the next box, we have to consider that the test might terminate. 
We define a parameter, $\beta$, that only appears in the limited trials. 
We let it be the loss the participant gets/feels when the test terminates before he or she is able to choose what the majority colour is.
%Recall that $\beta$ is the loss when the test terminates and that $T$ is the number of boxes that already are opened when the test terminates. 
Recall that $T$ is the number of boxes that already are opened when the test terminates. 
$T$ is uniformly distributed as in \eqref{T_uniform}. The loss when the test does not terminate will be the loss for when the next box is open, in the same way as for the unlimited trials. We can include the event of the test terminating as an indicator function, where an indicator function is as seen in \eqref{loss_func_indicator}. Thus, the loss function for opening the next box in an unlimited trial is the loss you get when the next box is opened plus $\alpha$ times an indicator function that is one if the test does not terminate. I addition to this we have the loss for when the test terminates, $\beta$, times an indicator for that the test terminates. Hence,
\begin{equation}
\label{loss_func_2_limited}
    \begin{aligned}
        L_i[Z,\delta_i=2] = \big( \alpha + L_i[Z,IO(x_{1:i},X_{i+1})] \big) \: I(T\neq i) + \beta \: I(T=i),
    \end{aligned}
\end{equation}
where $IO(x_{1:i},X_{i+1})$ are the choices that an Ideal Observer would do in the next steps. 

Now that we have all the loss functions, we can proceed with finding the expected losses. 

\subsection{Expected Losses}
When we find the expected losses we take the expectation of the loss functions as is done in \eqref{expectation_of_loss_func_general}. Recall that taking the expectation of an indicator function gives the probability of the event, and that $x_{1:i}$ is a vector containing the colours of the $i$ opened boxes.

As stated in Section \ref{section_notation}, the expected losses when $i$ boxes are opened, are denoted $\EE_{\delta_i}^i(\theta)$, with $\delta_i \in (0,1,2)$.
We start with the expected loss for choosing blue as the majority colour, $\EE^i_0(\theta)$. This is the same for both the unlimited and limited trials as the loss functions, stated in \eqref{loss_func_blue}, are identical. 
We condition on the colours of the opened boxes, as the true majority colour, $Z$, depends on the colours of all the twelve boxes. Thus,
\begin{equation}
\label{exp_loss_blue}
    \begin{aligned}
        \EE^i_{0}(\theta) = &E\big[ L_i[Z,\delta_i=0] \big |x_{1:i} ]\\
        = & E \big[ I(Z=1) \big|x_{1:i}]\\
        = & P(Z=1|x_{1:i}).
    \end{aligned}
\end{equation}
We see that the expected loss of choosing blue as the majority colour is equal to the probability that red is the majority colour given the colours of the opened boxes, for both the unlimited and limited versions. The only thing that this expected loss depends on are the colours of the first $i$ boxes, $x_{1:i}$. Thus, in this case, we have that $\theta = x_{1:i}$.
%\begin{equation*}
%    \theta = x_{1:i}.
%\end{equation*}

We find the expected loss of choosing red as the majority colour when $i$ boxes are opened, $\EE^i_{1}(\theta)$, similarly to \eqref{exp_loss_blue}. Again, conditioning on $x_{1:i}$, we get
\begin{equation}
\label{exp_loss_red}
    \begin{aligned}
        \EE^i_{1}(\theta) 
        = &E\big[ L_i[Z,\delta_i=1] \big |x_{1:i} ]\\
        = & E \big[ I(Z=0) \big|x_{1:i}]\\
        = & P(Z=0|x_{1:i}).
    \end{aligned}
\end{equation}
The expected loss for choosing red as the majority colour is then the probability that blue is the majority colour, conditioned on $x_{1:i}$, for both the unlimited and the limited case. As above, we see that $\theta = x_{1:i}$.

When we find the expected losses for opening the next box, we have to distinguish between the unlimited and limited cases. Staring with the unlimited case, we continue in the same way as for choosing blue or red as the majority colour, by taking the expectation of the loss function and conditioning on the colours of the $i$ opened boxes. Recall that $IO(x_{1:i},X_{i+1})$ are the choices that an Ideal Observer would do in the next steps. We then get that
\begin{equation*}
    \begin{aligned}
        \EE^i_{2}(\theta) 
        =& E \big[ \alpha + L_i[Z,IO(x_{1:i},X_{i+1})] |x_{1:i}\big].
    \end{aligned}
\end{equation*}
Taking the expectation of a constant gives the constant, thus $E[\alpha|x_{1:i}]=\alpha$, as $\alpha$ is not dependent on the colours of the boxes. Then,
\begin{equation}
\label{exp_loss_unlim_2_unfinished}
    \begin{aligned}
        \EE^i_{2}(\theta) 
        =& \alpha + E \big[L_i[Z,IO(x_{1:i},X_{i+1})] |x_{1:i}\big].
    \end{aligned}
\end{equation}
We see that $E \big[L_i[Z,IO(x_{1:i},X_{i+1})] |x_{1:i}\big]$ is the expected loss in the next step, and it depends on the colour of the box that opens, $X_{i+1}$. We find this expectation using the law of total expectation as in \eqref{law_tot_exp_func}. Then,
\begin{equation}
\label{exp_loss_next_unlim}
    \begin{aligned}
        E \big[L_i&[Z,IO(x_{1:i},X_{i+1})] |x_{1:i}\big] \\
        =& E \big[L_{i+1}[Z,IO(x_{1:i},X_{i+1})] |x_{1:i},X_{i+1}=0\big]P(X_{i+1}=0|x_{1:i}) \\
        +& E \big[L_{i+1}[Z,IO(x_{1:i},X_{i+1})] |x_{1:i},X_{i+1}=1\big]P(X_{i+1}=1|x_{1:i}),
    \end{aligned}
\end{equation}
where $P(X_{i+1}=0|x_{1:i})$ is the probability that box $i+1$ is blue given the colours of the first $i$ boxes and $P(X_{i+1}=1|x_{1:i})$ is the probability that it is red.  

%Eller skal jeg gjøre det sånn her: We see that $E \big[L[Z,\delta_{(i+1):12}] \big|x_{1:i}]$ is the expected loss in the next step, which we denote as $\EE_{\delta_{i+1},i+1}$. 

%Write about those $\delta_{i+1}$ actually being the IO choices. Thus, these are the choices in the future steps that have the lest expected loss. And this is how we find the exp loss in this step, thus, it depends on the IO solution. Call these decisions $IO(x_{1:i},0)$ if box $i+1$ is blue, and $IO(x_{1:i},1)$ if it is red. 

%Thus,
%\begin{equation*}
%    \EE_{2,i} = \alpha + \EE_{\delta_{i+1},i+1}.
%\end{equation*}
%$\EE_{\delta_{i+1},i+1}$ depends on the colour of the box that opens, $X_{i+1}$. Let $\EE_{\delta_{i+1},i+1|X_{i+1}=0}$ be the expected loss in the next step when the next box is blue, and $\EE_{\delta_{i+1},i+1|X_{i+1}=1}$ when box $i+1$ is red. We find the expectation using the law of total expectation as in \eqref{law_tot_exp_func}. Then,
%\begin{equation}
%\label{exp_loss_next_unlim}
%    \begin{aligned}
%        \EE_{\delta_{i+1},i+1}
%        = &\EE_{\delta_{i+1},i+1|X_{i+1}=0} P(X_{i+1}=0|x_{1:i})\\
%        + &\EE_{\delta_{i+1},i+1|X_{i+1}=1} P(X_{i+1}=1|x_{1:i}),
%    \end{aligned}
%\end{equation}
%where $P(X_{i+1}=0|x_{1:i})$ is the probability that box $i+1$ is blue given the colours of the first $i$ boxes and $P(X_{i+1}=1|x_{1:i})$ is the probability that it is red. 


Putting together \eqref{exp_loss_unlim_2_unfinished} and \eqref{exp_loss_next_unlim}, we get that the expected loss for opening the next box in the unlimited case is
\begin{equation}
    \begin{aligned}
        \EE^i_{2}(\theta) = \alpha 
        + &E \big[L_{i+1}[Z,IO(x_{1:i},X_{i+1})] |x_{1:i},X_{i+1}=0\big]P(X_{i+1}=0|x_{1:i}) \\
        +& E \big[L_{i+1}[Z,IO(x_{1:i},X_{i+1})] |x_{1:i},X_{i+1}=1\big]P(X_{i+1}=1|x_{1:i}).
    \end{aligned}
\end{equation}
We denote $E \big[L_{i+1}[Z,IO(x_{1:i},X_{i+1})] |x_{1:i},X_{i+1}\big]$ as $\EE^{i+1}_{IO(x_{1:i},X_{i+1})}$, such that
\begin{equation*}
    E \big[L_{i+1}[Z,IO(x_{1:i},X_{i+1})] |x_{1:i},X_{i+1}=0\big] = \EE^{i+1}_{IO(x_{1:i},0)}
\end{equation*}
and
\begin{equation*}
    E \big[L_{i+1}[Z,IO(x_{1:i},X_{i+1})] |x_{1:i},X_{i+1}=1\big] = \EE^{i+1}_{IO(x_{1:i},1)}.
\end{equation*}
The expression for the expected loss of opening the next box in the unlimited case is then
\begin{equation}
\label{exp_loss_next_box_unlim}
    \begin{aligned}
        \EE^i_{2}(\theta) 
        = \alpha + \EE^{i+1}_{IO(x_{1:i},0)} P(X_{i+1}=0|x_{1:i})
        + \EE^{i+1}_{IO(x_{1:i},1)} P(X_{i+1}=1|x_{1:i}).
    \end{aligned}
\end{equation}
In this case, the expected loss depends on both the colours of the opened boxes, $x_{1:i}$, and the parameter $\alpha$. Thus, $\theta = (\alpha,x_{1:i})$ (det er vel ikke riktig? x-ene er jo ikke parametere??).

We proceed in a similar manner when we find the expected loss of opening the next box in the limited case. Taking the expectation of the loss function in \eqref{loss_func_2_limited}, we get the expected loss when $i$ boxes are opened, $\EE^i_2(\theta)$. Recall that if the box terminates when $i$ boxes already are open, then the parameter $T$ is equal to $i$. We have to condition on $T$ being greater than or equal to $i$ when we find the expected loss, meaning that the test has not terminated yet when $i$ boxes are open. We then get that
\begin{equation}
\label{exp_loss_limited_a}
    \begin{aligned}
        \EE^i_{2}(\theta) 
        = &E\big[L_i[Z,\delta_i=2]\big|x_{1:i}] \\
        =& E\big[\big( \alpha + L_{i+1}[Z,IO(x_{1:i},X_{i+1})] \big) \: I(T\neq i) \\
        &+ \beta \: I(T=i) \: | \: x_{1:i}, T\geq i\big].
    \end{aligned}
\end{equation}
herfra må du fikse ting: 
We start with the first term in \eqref{exp_loss_limited_a}. The indicator function will be the probability of $T\neq i$, whereas for the expectation of the loss function when $i+1$ boxes are opened, we use the law of total expectation as in \eqref{law_tot_exp_func}, and condition on the colour of the next box, $X_{i+1}$. 

We also have that $\EE^{i+1}_{2}(\theta,X_{i+1}=j)$ is the expected loss in the next step given the colour of box $i+1$ and given that the test has not terminated yet. Thus,
\begin{equation}
\label{exp_loss_limited_b}
    \begin{aligned}
        E&\big[\big( \alpha + L_{i+1}[Z,IO(x_{1:i},X_{i+1})] \big) \: I(T\neq i)\big | x_{1:i}, T\geq i] \\
        =& \Big( \alpha + \sum_{j=0}^1 \EE^{i+1}_{2}(\theta,X_{i+1}=j) P(X_{i+1}=j|x_{1:i},T\geq i) \Big) 
        P(T \neq i)
    \end{aligned}
\end{equation}
Tregner jeg egentlig å ha $T\geq i$ i $P(X_{i+1}=j|x_{1:i},T\geq i)$???

The last term in \eqref{exp_loss_limited_a} becomes
\begin{equation}
\label{exp_loss_limited_c}
    \begin{aligned}
        E\big[ \beta \: I(T=i)|x_{1:i}, T\geq i\big] 
        = \beta \: P(T=i|T\geq i)
    \end{aligned}
\end{equation}
as it does not depend on the colours of the boxes, $x_{1:i}$.

Putting together \eqref{exp_loss_limited_a}, \eqref{exp_loss_limited_b} and \eqref{exp_loss_limited_c}, we get that the expected loss for opening another box in the limited case is
\begin{equation}
\label{exp_loss_limited_final}
    \begin{aligned}
        \EE^i_{2}(\theta) 
        =& \Big( \alpha + \sum_{j=0}^1 \EE^{i+1}_{2}(\theta,X_{i+1}=j) P(X_{i+1}=j|x_{1:i},T\geq i) \Big) \\[6pt]
        &\times P(T \neq i|T\geq i)\\[6pt]
        &+ \beta \: P(T=i|T\geq i).
    \end{aligned}
\end{equation}
This expected loss depends on $\alpha$, $\beta$ and $x_{1:i}$, thus in this case we have that $\theta = (\alpha,\beta,x_{1:i})$.

Now that we have expressions for the expected losses, we have to find the probabilities in these expressions. 

\subsection{Probabilities}
Say something here about which probabilities you will find. 

\subsubsection{The Majority Colour}
When we find the probabilities used in the expressions for the expected losses, we start with the expected loss for choosing blue as the majority colour, as given in \eqref{exp_loss_blue}. Then we need the probability
\begin{equation}
\label{prob_red_major_Z}
    P(Z=1|x_{1:i}).
\end{equation}
%$P(Z=1|x_{1:i})$. 
This is the probability that red is the majority colour, given the colours of the boxes that already are observed. Using definition of $Z$ as it is in \eqref{def_of_Z_2}, we can express \eqref{prob_red_major_Z} as the probability that $U_i+V_i>6$, or that $U_i+V_i \geq 7$, given the colours of the $i$ first boxes. However, we also need to condition on $U_i+V_i \neq 6$, as we know that one of the colours always is in majority, such that there will never be six blue and six red boxes all together. (burde jeg skrive dette lenger opp, der hvor jeg definerer loss funksjonen?). Thus, we find $P(U_i+V_i \geq 7 | x_{1:i},U_i+V_i \neq 6)$. As the order the boxes have been opened in is irrelevant here, and $U_i = \sum_{j=1}^i X_j$, we use $U_i=u_i$ instead of $x_{1:i}$, to be consistent with the other notation. Thus, we have that
\begin{equation}
\label{Z_to_U_and_V}
    P(Z=1|x_{1:i}) = P(U_i+V_i \geq 7 | U_i=u_i,U_i+V_i \neq 6).
\end{equation}

Using Bayes rule as described in \eqref{bayesrule}, we get that
\begin{equation}
\label{redmajor2}
    \begin{aligned}
        P(&U_i+V_i \geq 7 | U_i=u_i,U_i+V_i \neq 6) \\[6pt]
        &= \frac{P(U_i+V_i\neq6|U_i=u_i,U_i+V_i\geq7)P(U_i+V_i\geq7|U_i=u_i)}{P(U_i+V_i\neq6|U_i=u_i)}.
    \end{aligned}
\end{equation}
As 
\begin{equation*}
    P(U_i+V_i\neq6|U_i=u_i,U_i+V_i\geq7) = 1,
\end{equation*}
we get that
\begin{equation}
\label{redmajor3}
    \begin{aligned}
        P(U_i+V_i \geq 7 | U_i=u_i,U_i+V_i \neq 6)
        = \frac{P(U_i+V_i\geq7|U_i=u_i)}{P(U_i+V_i\neq6|U_i=u_i)}.
    \end{aligned}
\end{equation}


We have that
\begin{equation}
\label{redmajor1}
    \begin{aligned}
        P(U_i+V_i \geq 7 | U_i=u_i) 
        &= \sum_{j=7}^{12} P(U_i+V_i = j | U_i=u_i)\\[6pt]
    \end{aligned}
\end{equation}
Thus, to be able to find $P(U_i+V_i \geq j | U_i=u_i)$, we start with finding $P(U_i+V_i = j | U_i=u_i)$. Using the law of total probability as in \eqref{lawoftotprob}, and conditioning on $\theta$, we get
\begin{equation} 
\label{prob_red_major}
    \begin{aligned}
        P(U_i&+V_i = j | U_i=u_i) \\[6pt]
        =& \int_0^1 P(U_i+V_i = j | U_i=u_i, \Theta=\theta) f(\theta| U_i=u_i) \dd \theta \\[6pt]
        =& \int_0^1 P(V_i = j-u_i | \Theta=\theta) f(\theta| U_i=u_i) \dd \theta
        %\\[6pt]
        %=& \int_0^1 \binom{12-i}{j-u_i} p^{j-u_i}(1-p)^{12-i-(j-u_i)} 
        % \frac{1}{\text{B}(\gamma,\kappa)}p^{\gamma-1}(1-p)^{\kappa-1} dP \\[6pt]
        %=& \frac{1}{\text{B}(\gamma,\kappa)} \binom{12-i}{j-u_i} \int_0^1 p^{j-u_i+\gamma-1} (1-p)^{12-i-(j-u_i)+\kappa-1} dP.
    \end{aligned}
\end{equation}
Thus, we need to find $P(V_i = j-u_i | \Theta=\theta)$ and $f(\theta| U_i=u_i)$. 

As $V_i$ has a binomial distribution as in \eqref{U_V_binomal_distri}, we get that
\begin{equation*}
    P(V_i=j-u_i|\Theta=\theta)=\binom{12-i}{j-u_i}\theta^{j-u_i}(1-\theta)^{12-i-(j-u_i)}
\end{equation*}


We can find $f(\theta| U_i=u_i)$ using Bayes rule. Hence,
\begin{equation*}
    f(\theta| U_i=u_i) = \frac{P(U_i=u_i|\Theta=\theta)f(\theta)}{P(U_i=u_i)},
\end{equation*}
which is proportional to the numerator of the right hand side as in \eqref{posterior_proportional}. Using that $U_i|\Theta$ has a binomial distribution with probability mass function as in \eqref{ui_prob_mass}, and that $\theta$ has a Beta prior, with density function as in \eqref{beta_density_func}, we get that
\begin{equation*}
    \begin{aligned}
        f(\theta|U_i=u_i) 
        &\propto P(U_i=u_i|\Theta=\theta)f(\theta)\\[6pt] 
        &\propto \theta^{u_i}(1-\theta)^{i-u_i}\theta^{\gamma-1}(1-\theta)^{\kappa-1}\\[6pt]
        &= \theta^{u_i+\gamma-1}(1-\theta)^{i-u_i+\kappa-1}.
    \end{aligned}
\end{equation*}
This is proportional to a beta-distribution with parameters $u_i+\gamma$ and $i-u_i+\kappa$, hence we can conclude that
\begin{equation*}
    \Theta|U_i \sim \text{Beta}(u_i+\gamma,i-u_i+\kappa),
\end{equation*}
and therefore that 
\begin{equation}
\label{theta_given_ui}
    f(\theta|U_i=u_i) = \frac{1}{\text{B}(u_i+\gamma,i-u_i+\kappa)}\theta^{u_i+\gamma-1}(1-\theta)^{i-u_i+\kappa-1}.
\end{equation}

As we now have expressions for $P(V_i=j-u_i|\Theta=\theta)$ and $f(\theta|U_i=u_i)$, we can put this into \eqref{prob_red_major}.
\begin{equation}
\label{red_12_equal_j_a}
    \begin{aligned}
         P(&U_i+V_i = j | U_i=u_i) \\[6pt]
        =& \int_0^1 P(V_i = j-u_i | \Theta=\theta) P(\Theta=\theta| U_i=u_i) \dd \theta \\[6pt]
        =& \int_0^1 \binom{12-i}{j-u_i}\theta^{j-u_i}(1-\theta)^{12-i-(j-u_i)} \frac{\theta^{u_i+\gamma-1}(1-\theta)^{i-u_i+\kappa-1}}{\text{B}(u_i+\gamma,i-u_i+\kappa)} \dd \theta\\[6pt].
    \end{aligned}
\end{equation}
Taking the parts that do not depend on $\theta$ outside of the integral and summing the exponents of $\theta$ and $(1-\theta)$, we get that \eqref{red_12_equal_j_a} is
\begin{equation}
\label{red_12_equal_j}
    \begin{aligned}
         P(&U_i+V_i = j | U_i=u_i) \\[6pt]
        =& \frac{\binom{12-i}{j-u_i}}{\text{B}(u_i+\gamma,i-u_i+\kappa)} \\[6pt]
        &\times \int_0^1 
        \theta^{j-u_i+u_i+\gamma-1}(1-\theta)^{12-i-(j-u_i)+i-u_i+\kappa-1} \dd \theta\\[6pt]
        =& \frac{\binom{12-i}{j-u_i}}{\text{B}(u_i+\gamma,i-u_i+\kappa)} \int_0^1 
        \theta^{j+\gamma-1}(1-\theta)^{12-j+\kappa-1} \dd \theta.
    \end{aligned}
\end{equation}
The part inside the integral is proportional to a beta distribution with parameters $j+\gamma$ and $12-j+\kappa$. The integral of a distribution over the parameter space is one, hence
\begin{equation*}
    \int_0^1 \frac{1}{\text{B}(j+\gamma,12-j+\kappa)}\theta^{j+\gamma-1}(1-\theta)^{12-j+\kappa} \dd \theta = 1.
\end{equation*}
Therefore,
\begin{equation*}
    \int_0^1 \theta^{j+\gamma-1}(1-\theta)^{12-j+\kappa} \dd \theta = \text{B}(j+\gamma,12-j+\kappa).
\end{equation*}
Putting this into \eqref{red_12_equal_j}, we get
\begin{equation}
\label{red_12_equal_j_final}
    \begin{aligned}
        P(U_i+&V_i = j | U_i=u_i) = \binom{12-i}{j-u_i} \frac{\text{B}(j+\gamma,12-j+\kappa)}{\text{B}(u_i+\gamma,i-u_i+\kappa)}.
    \end{aligned}
\end{equation}

Putting \eqref{red_12_equal_j_final} into \eqref{redmajor1}, we get that
\begin{equation}
\label{redmajor_given_u}
%\label{redmajor2}
    \begin{aligned}
        P(U_i+V_i \geq 7 | U_i=u_i) 
        &= \sum_{j=7}^{12} \binom{12-i}{j-u_i} \frac{\text{B}(j+\gamma,12-j+\kappa)}{\text{B}(u_i+\gamma,i-u_i+\kappa)}.
    \end{aligned}
\end{equation}

We have that
\begin{equation*}
    P(U_i+V_i\neq6|U_i=u_i,U_i+V_i\geq7)=1
\end{equation*}
and, using \eqref{red_12_equal_j_final}, we get 
\begin{equation}
\label{u12neq6}
    \begin{aligned}
        P(U_i+V_i\neq6|U_i=u_i) 
        &= 1-P(U_i+V_i=6|U_i=u_i)\\[6pt]
        &= 1-\binom{12-i}{6-u_i} \frac{\text{B}(6+\gamma,12-6+\kappa)}{\text{B}(u_i+\gamma,i-u_i+\kappa)}\\[6pt]
        &= 1-\binom{12-i}{6-u_i} \frac{\text{B}(6+\gamma,6+\kappa)}{\text{B}(u_i+\gamma,i-u_i+\kappa)}.
    \end{aligned}
\end{equation}
Putting \eqref{u12neq6} and \eqref{redmajor_given_u} into \eqref{redmajor3}, we get
\begin{equation}
\label{redmajor_final}
    \begin{aligned}
        P(U_i+V_i \geq 7 | U_i=u_i,U_i+V_i \neq 6) 
        &= \frac{\sum_{j=7}^{12} \binom{12-i}{j-u_i} \frac{\text{B}(j+\gamma,12-j+\kappa)}{\text{B}(u_i+\gamma,i-u_i+\kappa)}}{1-\binom{12-i}{6-u_i} \frac{\text{B}(6+\gamma,6+\kappa)}{\text{B}(u_i+\gamma,i-u_i+\kappa)}}.
    \end{aligned}
\end{equation}
This is the probability that there is a red majority in total, given the colour of the first $i$ boxes that are opened, and given that one of the colours is in majority. 


In the expected loss for choosing red as the majority colour, we have $P(Z=0|x_{1:i})$, as in \eqref{exp_loss_red}. The same argument holds here as in \eqref{Z_to_U_and_V}. Thus, we have that
\begin{equation}
    P(Z=0|x_{1:i}) = P(U_i+V_i \leq 5 | U_i=u_i,U_i+V_i \neq 6).
\end{equation}
This is the probability that blue is the majority colour, which is the complementing probability to the probability that red is the majority colour. Therefore,

\begin{equation}
\label{blue_major}
    \begin{aligned}
        P(U_i+V_i& \leq 5 | U_i=u_i,U_i+V_i \neq 6) \\
        &= 1 - P(U_i+V_i \geq 7 | U_i=u_i,U_i+V_i \neq 6).
    \end{aligned}
\end{equation}
Putting the expression in \eqref{redmajor_final} into \eqref{blue_major}, we get that the probability of blue being the dominant colour is
\begin{equation}
\label{blue_major_final}
    \begin{aligned}
        P(U_i+V_i& \leq 5 | U_i=u_i,U_i+V_i \neq 6) 
        = 1 - \frac{\sum_{j=7}^{12} \binom{12-i}{j-u_i} \frac{\text{B}(j+\gamma,12-j+\kappa)}{\text{B}(u_i+\gamma,i-u_i+\kappa)}}{1-\binom{12-i}{6-u_i} \frac{\text{B}(6+\gamma,6+\kappa)}{\text{B}(u_i+\gamma,i-u_i+\kappa)}}.
    \end{aligned}
\end{equation}

\subsubsection{The Colour of the Next Box}
Next, we have a look at the expected losses for opening the next box, both in the unlimited and the limited cases, as given in \eqref{exp_loss_next_box_unlim} and \eqref{exp_loss_limited_final}, respectively. In both of these expression we have the probability that the next box is either red or blue, given the colours of the boxes that are opened and given that the test has not terminated yet. This is given as $P(X_{i+1}=1|x_{1:i},T\geq i)$ and $P(X_{i+1}=0|x_{1:i},T\geq i)$, respectively.

We start with $P(X_{i+1}=1|x_{1:i},T\geq i)$, and again we change the notation from $x_{1:i}$ to $U_i=u_i$ and $V_i=v_i$, with the same argument as for \eqref{Z_to_U_and_V}. Thus,
her dropper jeg alle $T \geq i$, for jeg er ikke helt sikker på at jeg skal ha det med. 
\begin{equation}
    \begin{aligned}
        P(X_{i+1}=1|x_{1:i}) = P(X_{i+1}=1|U_i=u_i,U_i+V_i\neq6)
    \end{aligned}
\end{equation}
Using Bayes' rule we get that this is
\begin{equation}
\label{nextisred_bayes_rule}
    \begin{aligned}
        P(&X_{i+1}=1|U_i=u_i,U_i+V_i\neq6) \\[6pt]
        &= \frac{P(U_i+V_i\neq6|U_i=u_i,X_{i+1}=1)P(X_{i+1}=1|U_i=u_i)}
        {P(U_i+V_i\neq6|U_i=u_i)},
    \end{aligned}
\end{equation}

where the expression in the denominator, $P(U_i+V_i\neq6|U_i=u_i)$ is as given in \eqref{u12neq6}. 

We start by finding $P(X_{i+1}=1|U_i=u_i)$. Using the law of total probability as given in \eqref{lawoftotprob}, we get
\begin{equation}
\label{xiplus1_given_ui1}
    \begin{aligned}
        P(X_{i+1}=1|U_i=u_i)
        = \int_0^1 P(X_{i+1}=1|U_i=u_i,\Theta=\theta)f(\theta|U_i=u_i) \dd \theta.
    \end{aligned}
\end{equation}
The expression for $f(\theta|U_i=u_i)$ is as given in \eqref{theta_given_ui}. All of the $x$'s are Bernoulli distributed with probability $\theta$. They are conditionally independent of each other, given $\theta$. Therefore, the probability that $X_{i+1}$ is one, or red, is independent of the colour of the of the boxes that already are opened. The probability that a box that is opened is 1 is also equal to $\theta$. Hence,
\begin{equation}
\label{nextisred_equal_theta}
    \begin{aligned}
        P(X_{i+1}=1|U_i=u_i,\Theta=\theta) = P(X_{i+1}=1|\Theta=\theta) = \theta.
    \end{aligned}
\end{equation}
Putting \eqref{nextisred_equal_theta} and \eqref{theta_given_ui} into \eqref{xiplus1_given_ui1} gives
\begin{equation}
\label{xiplus1_given_ui2}
    \begin{aligned}
        P(&X_{i+1}=1|U_i=u_i)\\
        &= \int_0^1 \theta \frac{1}{\text{B}(u_i+\gamma,i-u_i+\kappa)}\theta^{u_i+\gamma-1}(1-\theta)^{i-u_i+\kappa-1}  \dd \theta\\[6pt]
        &=\frac{1}{\text{B}(u_i+\gamma,i-u_i+\kappa)} \int_0^1 \theta^{u_i+\gamma+1-1}(1-\theta)^{i-u_i+\kappa-1} \dd \theta.
    \end{aligned}
\end{equation}
Again, the part inside the integral is proportional to a beta distribution, here with parameters $u_i+\gamma+1$ and $i-u_i+\kappa$. Integrating a distribution over the parameter space gives one, which in this case gives
\begin{equation*}
    \begin{aligned}
        \int_0^1 \frac{1}{\text{B}(u_i+\gamma+1,i-u_i+\kappa)} \theta^{u_i+\gamma+1-1}(1-\theta)^{i-u_i+\kappa-1}  \dd \theta = 1.
    \end{aligned}
\end{equation*}
Hence,
\begin{equation}
\label{integral_of_beta_distr}
    \begin{aligned}
        \int_0^1 \theta^{u_i+\gamma+1-1}(1-\theta)^{i-u_i+\kappa-1} \dd \theta = \text{B}(u_i+\gamma+1,i-u_i+\kappa).
    \end{aligned}
\end{equation}
Inserting \eqref{integral_of_beta_distr} into \eqref{xiplus1_given_ui2} gives
\begin{equation}
\label{xiplus1_given_ui3}
    \begin{aligned}
        P(X_{i+1}=1|U_i=u_i) 
        = \frac{\text{B}(u_i+\gamma+1,i-u_i+\kappa)}
        {\text{B}(u_i+\gamma,i-u_i+\kappa)}.
    \end{aligned}
\end{equation}
Using the property of the beta function as stated in \eqref{beta_as_gamma}, we get that the numerator in \eqref{xiplus1_given_ui3} is
\begin{equation}
\label{beta_to_gamma1}
     \text{B}(u_i+\gamma+1,i-u_i+\kappa)
     =\frac{\Gamma(u_i+\gamma+1)\Gamma(i-u_i+\kappa)}{\Gamma(u_i+\gamma+1+i-u_i+\kappa)},
\end{equation}
and that the denominator is
\begin{equation}
\label{beta_to_gamma2}
    \text{B}(u_i+\gamma,i-u_i+\kappa)
    =\frac{\Gamma(u_i+\gamma)\Gamma(i-u_i+\kappa)}{\Gamma(u_i+\gamma+i-u_i+\kappa)}.
\end{equation}
Inserting \eqref{beta_to_gamma1} and \eqref{beta_to_gamma2} into \eqref{xiplus1_given_ui3}, gives
\begin{equation}
\label{xiplus1_given_ui4}
    \begin{aligned}
        P(X_{i+1}=1|U_i=u_i) 
        &= \frac{\frac{\Gamma(u_i+\gamma+1)\Gamma(i-u_i+\kappa)}{\Gamma(u_i+\gamma+1+i-u_i+\kappa)}}
        {\frac{\Gamma(u_i+\gamma)\Gamma(i-u_i+\kappa)}{\Gamma(u_i+\gamma+i-u_i+\kappa)}} \\[6pt]
        &= \frac{\frac{\Gamma(u_i+\gamma+1)}{\Gamma(\gamma+1+i+\kappa)}}
        {\frac{\Gamma(u_i+\gamma)}{\Gamma(\gamma+i+\kappa)}}.
    \end{aligned}
\end{equation}
Using the recursive property of the gamma function as seen in \eqref{gamma_recursive_property}, we get that the nominator in \eqref{xiplus1_given_ui4} is
\begin{equation}
\label{recursive_gamma}
    \frac{\Gamma(u_i+\gamma+1)}{\Gamma(\gamma+1+i+\kappa)}
    =\frac{(\gamma+u_i)\Gamma(u_i+\gamma)}{(\gamma+\kappa+i)\Gamma(\gamma+i+\kappa)}.
\end{equation}
Inserting \eqref{recursive_gamma} into \eqref{xiplus1_given_ui4}, we get
\begin{equation}
\label{xiplus1_given_ui5}
    \begin{aligned}
        P(X_{i+1}=1|U_i=u_i) 
        &= \frac{\frac{(\gamma+u_i)\Gamma(u_i+\gamma)}{(\gamma+\kappa+i)\Gamma(\gamma+i+\kappa)}}
        {\frac{\Gamma(u_i+\gamma)}{\Gamma(\gamma+i+\kappa)}}\\[6pt]
        &=\frac{\gamma+u_i}
        {\gamma+\kappa+i}.
    \end{aligned}
\end{equation}

In the expression in \eqref{nextisred_bayes_rule}, it remains to find $P(U_i+V_i\neq6|U_i=u_i,X_{i+1}=1)$.
Firstly, (er dette riktig?)
\begin{equation*}
    \begin{aligned}
        P(U_i+V_i\neq6|U_i=u_i,X_{i+1}=1) 
        &= P(U_i+V_i\neq6|U_{i+1}=u_i+1)\\[6pt]
        &= P(U_{i+1}+V_{i+1}\neq6|U_{i+1}=u_i+1).
    \end{aligned}
\end{equation*}
From \eqref{u12neq6} we have that
\begin{equation}
    \begin{aligned}
        P(U_i+V_i\neq6|U_i=u_i) 
        &= 1-\binom{12-i}{6-u_i} \frac{\text{B}(6+\gamma,6+\kappa)}{\text{B}(u_i+\gamma,i-u_i+\kappa)}.
    \end{aligned}
\end{equation}
Putting $i+1$ into this instead of $i$ and $u_i+1$ instead of $u_i$, we get that
\begin{equation}
\label{u12neq6_givennextisred}
    \begin{aligned}
        P&(U_i+V_i\neq6|U_i=u_i,X_{i+1}=1)\\[6pt] 
        &= P(U_i+V_i\neq6|U_{i+1}=u_i+1)\\[6pt]
        &= 1-\binom{12-(i+1)}{6-(u_i+1)} \frac{\text{B}(6+\gamma,6+\kappa)}{\text{B}(u_i+1+\gamma,i+1-(u_i+1)+\kappa)}\\[6pt]
        &= 1-\binom{11-i}{5-u_i} \frac{\text{B}(6+\gamma,6+\kappa)}{\text{B}(u_i+\gamma+1,i-u_i+\kappa)}
    \end{aligned}
\end{equation}

Inserting \eqref{u12neq6_givennextisred}, \eqref{xiplus1_given_ui5} and \eqref{u12neq6} into \eqref{nextisred_bayes_rule}, we get
\begin{equation}
\label{nextisred_given_majority}
    \begin{aligned}
        P(&X_{i+1}=1|U_i=u_i,U_i+V_i\neq6) \\[6pt]
        &= \frac{P(U_i+V_i\neq6|U_i=u_i,X_{i+1}=1)P(X_{i+1}=1|U_i=u_i)}
        {P(U_i+V_i\neq6|U_i=u_i)}\\[6pt]
        &= \frac{\bigg[ 1 - \binom{11-i}{5-u_i}\frac{\text{B}(6+\gamma,6+\kappa)}{\text{B}(\gamma+u_i+1,\kappa+i-u_i)} \bigg]
        \frac{\gamma+u_i}
        {\gamma+\kappa+i}}
        {1-\binom{12-i}{6-u_i} \frac{\text{B}(6+\gamma,6+\kappa)}{\text{B}(u_i+\gamma,i-u_i+\kappa)}}.
    \end{aligned}
\end{equation}

As we now have the probability that the net box that is opened is red, we can find the probability that the next box that opens is blue. These are complementary probabilities as there are only two possible colours each box can be. 

Using the same arguments as before, we have that
\begin{equation}
    \begin{aligned}
        P(X_{i+1}=0|x_{1:i}) = P(X_{i+1}=0|U_i=u_i,U_i+V_i\neq6).
    \end{aligned}
\end{equation}
As this probability is the compliment of $P(X_{i+1}=1|U_i=u_i,U_i+V_i\neq6)$, we get that
\begin{equation}
\label{next_is_blue_complement}
    \begin{aligned}
        P(X_{i+1}=0|U_i=u_i,U_i+V_i\neq6) = 1 - P(X_{i+1}=1|U_i=u_i,U_i+V_i\neq6).
    \end{aligned}
\end{equation}
Trenger jeg å skrive denne sannsynligheten videre ut? for det jeg egentlig gjør er å finne sannsynligheten for rød, også bare bruke \eqref{next_is_blue_complement} for å finne sanns for blå.


\subsubsection{When the Test Terminates}
In the expected loss for opening another box in the unlimited case, as shown in \eqref{exp_loss_limited_final}, we also have the probabilities that the test terminates when $i$ boxes are opened given that the test has not terminated yet, or that it does not. These are $P(T=i|T\geq i)$ and $P(T\neq i|T\geq i)$. These are complementary probabilities, such that
\begin{equation}
\label{T_neq_i}
    P(T \neq i|T\geq i) = 1-P(T=i|T\geq i).
\end{equation}
Thus, if we find $P(T=i|T\geq i)$, we can easily find $P(T\neq i|T\geq i)$ using \eqref{T_neq_i}. 

Using Bayes' rule as it is given in \eqref{bayesrule}, we get
\begin{equation}
\label{T_equal_i_a}
    \begin{aligned}
        P(T=i|T\geq i) = \frac{P(T\geq i|T=i)P(T=i)}{P(T\geq i)}.
    \end{aligned}
\end{equation}
We see that 
\begin{equation}
\label{T_geq_i_given_i}
    P(T\geq i|T=i) = 1.
\end{equation}
As $T$ is uniformly distributed with 11 possible values, as in \eqref{T_uniform}, we have that
\begin{equation}
\label{prob_T_equal_i}
    P(T=i) = \frac{1}{11}.
\end{equation}
It then remains to find $P(T\geq i)$. We have that
\begin{equation}
\label{T_geq_i_unfinished}
    \begin{aligned}
        P(T\geq i) = 1 - P(T<i) = 1 - \sum_{j=1}^{i-1}P(T=j).
    \end{aligned}
\end{equation}
As in \eqref{prob_T_equal_i}, we have that $P(T=j)$ is $\frac{1}{11}$. Thus, \eqref{T_geq_i_unfinished} becomes
\begin{equation}
\label{T_geq_i}
    \begin{aligned}
        P(T\geq i) =& 1-\sum_{j=1}^{i-1} \frac{1}{11} = 1- (i-1)\frac{1}{11}\\[6pt]
        =& \frac{11-(i-1)}{1} = \frac{12-i}{11}.
    \end{aligned}
\end{equation}
Inserting \eqref{T_geq_i_given_i}, \eqref{prob_T_equal_i} and \eqref{T_geq_i} into \eqref{T_equal_i_a}, we get 
\begin{equation}
    \begin{aligned}
        P(T=i|T\geq i) = \frac{\frac{1}{11}}{\frac{12-i}{11}} = \frac{1}{12-i}.
    \end{aligned}
\end{equation}

We now have all that we need to find the tree expected losses each time a box is opened in both unlimited and limited trials. When we find the IO solution, we always choose the decision with the least expected loss. 





\section{Maximum Likelihood Estimators}
As we have defined a model for the decisions that the participants make and found expressions for the expected losses for each of the three decisions, we can now fit the model to each participant. We do this by finding maximum likelihood estimates of $\alpha$, $\beta$ and $\eta$ based on the decisions the participants have made already. These decisions are found in the data as described in the introduction of this chapter. 

The model, with the three parameters, now looks like this
\begin{equation}
\label{softmax_real}
    f(\delta_{i}|\alpha, \beta,\eta) = \frac{\text{exp}(- \eta \EE^i_{\delta_{i}}(\alpha, \beta))}{\sum_{d=0}^{2} \text{exp}(-\eta \EE^i_{d}(\alpha, \beta))}. 
\end{equation}

For each participants we have observed decisions, $(\delta_1,\delta_2,...,\delta_n)$, where $\delta \in \{0,1,2 \}$. (må skirve om dataen i begynnelsen av kap før du skriver dette)

Hva er likelihood? og hvordan er notasjonen for det? også hvordan vi finner det. 

We find the likelihood as in \eqref{likelihood}. 


Log-likelihood function. og om at det er det samme om vi maksimerer log likel eller likelihood funcs. 

the MLEs are the values of teh parameters that minimizes teh log likleihood. How did you find that? numerically using L-BFGS. write really short about that. 


\section{Confidence Intervals for the Parameters}