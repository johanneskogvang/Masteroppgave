\newpage
%\section{Problem Setup}
\chapter{Model formulation}
\label{Chapter_Problem_setup}
The box task is an information sampling task used to assess a 'jumping to conclusions' (JTC) bias \citep{balzan2017}. In the box task used in this report, the participants are shown a grid of twelve boxes, and each time a box is opened, one out of two colours, for example, blue or red, is displayed. Participants are told that one colour is always in the majority and that their task is to find out which one. We use two different versions of the box task. In the first one, the participants can open as many of the twelve boxes as they want before deciding which of the two colours is in the majority. We call this the unlimited version. In the second one, which we call the limited version, the participants are informed that the test will terminate at one point when a random box is opened. 
If the participant has not decided what the majority colour is when the test terminates, this counts as a failed trial. 
The participant could, for example, try to open the fourth box when the test terminates. Then, she does not get to see what colour that fourth box has. She cannot choose what she thinks is the majority colour, and this is a failed trial.

We have data from 76 participants that have done multiple trials of both versions of the box task. The experiment where the data was collected was carried out by Professor Gerit Pfuhl and Doctoral Research Fellow Kristoffer Klevjer at UiT The Arctic University of Norway in February 2020. They recruited participants from an undergraduate psychology course. First, they did a practice trial that was a limited trial that terminated after opening three boxes. That means that if they tried to open the fourth, the test terminated, and they could not make a decision. That trial is not analyzed here. Following the practice trial were three unlimited trials. The participants could, in these trials, open as many of the twelve boxes as they wanted before deciding on what they think is the dominant colourâ€”lastly, there were six limited trials. Three of them terminated after the participants had opened six boxes, and the other three terminated after they had opened nine boxes. These nine trials are the ones we analyze here, meaning that we analyze Trials 2 to 10. We have data for how many boxes each participant has opened in each of the nine trials. We call this 'draws to decision'. The participants have either opened boxes until they have decided what they think is the majority colour or until the test terminates. We have data for what they chose or whether the test terminated before they were able to choose. To compensate for possible biases towards one colour, the two colours were changed for each new trial. They could, for example, be green and pink in the first trial and blue and yellow in the second trial. For simplicity, we are in this report referring to these colour as blue and red for all trials.

For each trial, there is a fixed sequence of boxes. The participants can only choose whether to open the next box or not; they cannot choose which box they open. Thus, we know how many of the boxes that were blue and how many that were red for each step in the different trials. In Trial 2, which is an unlimited trial, the boxes were opened in the order that is shown in Figure \ref{fig:trial2_order}. In Figure \ref{fig:histogram_trial2}, the draws to decisions for all participants are shown in a histogram. Here, the number of boxes that are opened when the participant chooses what she thinks is the majority colour is on the horizontal axis. On the vertical axis are the number of participants that have decided on that particular box. We see that many participants have chosen the majority colour after they have opened three boxes. All of these three boxes are red. Thus, there is a high probability that red is the dominant colour. 
As the participants are told that one of the colours is always in the majority, we can be completely sure if six of the opened boxes are red, that red is the dominant colour. This is because there cannot be six of each box if one of them is in the majority. When seven boxes are opened in Trial 2, six of them are red, and we know then that red is the dominant colour. Seven participants have chosen colour after seven boxes are opened. Some participants wait longer, even though they can be completely sure after seven boxes are opened.
\begin{figure}
    \centering
    \scalebox{0.8}{\input{visualize_trials_tikz/trial2}}
    %\includegraphics{}
    \caption[Order of Boxes in Trial 2]{The order of the boxes in Trial 2. This is an unlimited trial.}
    \label{fig:trial2_order}
\end{figure}


\begin{figure}
    \centering
    \includegraphics[scale=0.6]{pictures/dtd2_histogram.pdf}
    \caption[Draws to Decisions in Trial 2]{Histogram of the draws to decisions for all participants in Trial 2.}
    \label{fig:histogram_trial2}
\end{figure}


In Trial 3, which is also an unlimited trial, it takes more boxes to be completely sure what the majority colour is. As shown in Figure \ref{fig:trial3_order}, there are six blue boxes when ten of the boxes are opened. We see in Figure \ref{fig:histogram_trial3} that the participants in general open more boxes before choosing the majority colour in this trial than in Trial 2. 

\begin{figure}
    \centering
    \scalebox{0.8}{\input{visualize_trials_tikz/trial3}}
    %\includegraphics{}
    \caption[Order of Boxes in Trial 3]{The order of the boxes in Trial 3. This is an unlimited trial.}
    \label{fig:trial3_order}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[scale=0.6]{pictures/dtd3_histogram.pdf}
    \caption[Draws to Decisions in Trial 3]{Histogram of the draws to decisions for all participants in Trial 3.}
    \label{fig:histogram_trial3}
\end{figure}

Both Trial 5 and Trial 8 are limited trials that terminate after nine boxes are opened. The order of the boxes in Trial 5 is shown in Figure \ref{fig:trial5_order}. When seven boxes are opened, six of them are blue, and we can therefore conclude when seven boxes are opened that blue is the majority colour. We see in Figure \ref{fig:histogram_trial5} that many participants choose the majority colour after three boxes are opened. All of these three are blue boxes. In Trial 8, there are never two boxes of the same colour following each other, as shown in Figure \ref{fig:trial8_order}. There are at no point in this trial six of one of the colours, meaning that we never can be completely sure which one is the majority colour. This is reflected in the draws to decision for the participants, as shown in Figure \ref{fig:histogram_trial8}. We see that the test terminates for many of the participants before choosing what they think is the majority colour.  
\begin{figure}
    \centering
    \scalebox{0.8}{\input{visualize_trials_tikz/trial5}}
    \caption[Order of Boxes in Trial 5]{The order of the boxes in Trial 5. This is a limited trial that terminates after nine boxes are opened.}
    \label{fig:trial5_order}
\end{figure}
\begin{figure}
    \centering
    \scalebox{0.8}{\input{visualize_trials_tikz/trial8}}
    \caption[Order of Boxes in Trial 8]{The order of the boxes in Trial 8. This is a limited trial that terminates after nine boxes are opened.}
    \label{fig:trial8_order}
\end{figure}
\begin{figure}
    \centering
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[scale=0.4]{pictures/dtd5_histogram.pdf}
        \caption[Draws to Decisions in Trial 5]{The draws to decisions for all participants in Trial 5. That is, how many boxes they open before they choose what they think is the majority colour, or before the test terminates.}
        \label{fig:histogram_trial5}
    \end{minipage}\hfill
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[scale=0.4]{pictures/dtd8_histogram.pdf}
        \caption[Draws to Decisions in Trial 8]{The draws to decisions for all participants in Trial 8. That is, how many boxes they open before they choose what they think is the majority colour, or before the test terminates.}
        \label{fig:histogram_trial8}
    \end{minipage}
\end{figure}

The order of the boxes for all trials can be found in Appendix \ref{appendix_a}. Here we also have histograms of the draws to decision for all of the trials.

In the following, we formulate a model for how the participants make decisions in the box task, and we estimate parameters such that we can fit the model to each person. We also find a so-called Ideal Observer solution of the box task. An Ideal Observer would always make optimal decisions \citep{idealObs}. Thus, an Ideal Observer solution is close to an optimal solution of the box task.  



\section{Modelling Framework}
\label{section_notation}
Before we start with the formulation of the model, we will introduce some notation and present some assumptions. 

We let $X_i$ be the colour of the $i$-th opened box as in \eqref{def_of_Xi}. Then, if the box is blue, $X_i$ is zero, and if the box is red, $X_i$ is one.
We assume that each $X_i$ has a Bernoulli distribution with success probability $\Theta$, where we later condition on there not being six blue and six red boxes. Then,
\begin{equation*}
    X_i \sim \text{Bernoulli}(\Theta).
\end{equation*}
We also define a vector, $X_{1:i}$, that contains the colours of the first $i$ boxes that are or will be opened, just as in \eqref{def_of_X1:i}. In the same way, we let $x_{1:i} = (x_1,x_2,...,x_{i})$.

Additionally, let $U_i$ be the number of the first $i$ opened boxes that are red. Thus, $U_i$ is a stochastic variable defined as
\begin{equation}
\label{def_of_U}
    U_i = \sum_{j=1}^{i} X_j. 
\end{equation}
The sum of Bernoulli distributed variables is binomially distributed \citep{statinf}. Thus, $U_i$ is binomially distributed with parameters $i$ and $\Theta$. We define another stochastic variable, $V_i$, that is the number of red boxes that are not opened when $i$ boxes are opened. Thus, $V_i$ is the number of red boxes out of the $12-i$ boxes that are not opened, which yields,
\begin{equation}
\label{def_of_V}
    V_i = \sum_{j=i+1}^{12} X_j.
\end{equation}
This variable is also binomially distributed, but with parameters $12-i$ and $\Theta$. Thus,
\begin{equation}
\label{U_V_binomal_distri}
    \begin{aligned}
        U_i &\sim \text{Binomial}(i,\Theta)\\
        V_i &\sim \text{Binomial}(12-i,\Theta).
    \end{aligned}
\end{equation}
%\begin{subequations}
%\begin{equation}
%\label{U_binomail_distr}
%    U_i \sim \text{Binomial}(i,\theta)
%\end{equation}
%\begin{equation}
%\label{V_binomial_distr}
%    V_i \sim \text{Binomial}(12-i,\theta).
%\end{equation}
%\end{subequations}
Then, we have that 
\begin{equation}
\label{ui_prob_mass}
    P(U_i=u_i|\Theta=\theta) = \binom{12}{u_i} \theta^{u_i}(1-\theta)^{12-u_i},
\end{equation}
and
\begin{equation}
\label{vi_prob_mass}
    P(V_i=v_i|\Theta=\theta) = \binom{12-i}{v_i} \theta^{v_i}(1-\theta)^{12-i-v_i}.
\end{equation}

Just as in Chapter \ref{theory_bayesian_modelling}, we let $\Theta$ have a conjugate beta prior with parameters $\gamma$ and $\kappa$, as shown in \eqref{theta_with_beta_prior}.
The prior distribution of $\Theta$ is then as given in \eqref{betadistribution}.

Figure \ref{fig:pdf_beta_distr} shows the probability density function of the beta distribution for different values of $\gamma$ and $\kappa$. The pink line represents the situation where $\gamma=\kappa=1$. This is the same as having a uniform prior for $\Theta$. That means that the probability of $\Theta$ being anywhere on the interval between zero and one is constant. As the participants are told that one of the colours will be in the majority but get no information about which one, this might be a suitable prior.
However, one might argue that our prior beliefs resemble the purple or orange lines as we know that one of the colours will definitively be in the majority. Thus it might not be reasonable to assume that $\Theta$ is 0.5. For this reason, we exclude all priors that have $\gamma$ and $\kappa$ larger than 1, which is the situation for the black and grey lines. 

\begin{figure}
    \centering
    \includegraphics[scale=0.5]{pictures/beta_pdf.pdf}
    \caption[Probability Density for the Beta Distribution]{The probability density function for the beta distribution plotted for different values of the hyperparameters $\gamma$ and $\kappa$.}
    \label{fig:pdf_beta_distr}
\end{figure}


Of all the 12 boxes, $U_i+V_i$ is the total number of red boxes.
Consequently, if $U_i+V_i$ is bigger than 6, it is a red majority in the box task, and if it is smaller than 6, the true majority colour is blue. We denote this true majority colour as $Z$, such that
\begin{equation}
\label{def_of_Z_2}
    Z = I(U_i+V_i>6).
\end{equation}
This is the same as defining $Z$ as in \eqref{def_of_Z}, as $U_i+V_i = \sum_{j=1}^{12}X_j$, and the order that the boxes are opened in does not affect the majority. Then, as in \eqref{Z_true_majority}, $Z$ is zero if the true majority colour is blue and one if the true majority colour is red. 


%Each time a box is opened, we have three choices. The first is to choose that blue is the majority colour, the second that red is and the third is to open another box. Let $\delta_{i+1}$ denote the different choices when $i$ boxes are opened. If $\delta_{i+1}=0$, we have chosen that blue is the majority colour, $\delta_{i+1}=1$ shows that red is chosen as the majority and $\delta_{i+1}=2$ represents the choice of opening the next box, which is box $i+1$. Moreover, let $\delta_{(i+2):12}$ be the choices made after box $i+1$ is opened. 

Each time a box is opened, the participants have three choices. The first is to choose blue as the dominant colour, the second that red is, and the third is to choose to open another box. We denote these decisions as $\delta_i$, where $i$ is the number of opened boxes. If $\delta_i = 0$, the participant chooses that blue is the more prominent colour, thus that there are in total, of all twelve boxes, more blue boxes than red. Moreover, $\delta_i=1$ means that the participant has chosen that red is the dominant colour, and $\delta_i=2$ represents the situation where the participant chooses to open the next box. Thus,
\begin{equation}
\label{def_of_delta}
    \delta_i =
    \begin{cases}
        0,& \quad \text{if blue is chosen as majority colour,}\\
        1,& \quad \text{if red is chosen as majority colour,}\\
        2,& \quad \text{if the participant chooses to open the next box.}
    \end{cases}
\end{equation}

We can define loss functions for each of these decisions. These loss functions depend on the true majority colour, $Z$, and the decision, $\delta_i$. Similarly to Chapter \ref{theory_loss_functions}, we denote the loss function when $i$ boxes are opened as $L_i[Z,\delta_i;\vp]$. In our case, we have that 
\begin{equation*}
    \Omega_Z = \{0,1\}
\end{equation*}
and 
\begin{equation*}
    \Omega_{\delta_i}=\{0,1,2\}.
\end{equation*}
%$\Omega_Z = \{0,1\}$ and $\Omega_{\delta_i}=\{0,1,2\}$.


If we take the expectation of this loss function, we get the expected loss for each of these decisions when $i$ boxes are opened, which we denote
\begin{equation*}
    \EE^i_{\delta_i}(\vp) = E\left[ L_i[Z,\delta_i;\vp]|X_{1:i}=x_{1:i}\right],
\end{equation*}
as it depends on some parameters $\vp$. This expected loss also depends on the colours of the $i$ boxes that already are opened, $x_{1:i}$.

In the limited version of the box task, the participants are told that the test will terminate when a random box is opened. Thus, we need a random variable representing how many boxes that are open when the test terminates. We call this variable $T$. If $T=3$, the participant has opened three boxes and wants to open the fourth when the test terminates. Then, instead of seeing the colour of the fourth box, the test terminates, and this is a failed trial. The information given to the participants regarding this is that the test will terminate when a random box is opened. We assume that the first box can always be opened, but the probabilities that the test terminates at the subsequent boxes are the same. When 12 boxes are opened, there are no more boxes to open and, therefore, no more chances for the test to terminate. Thus, $T$ is uniformly distributed with on $\{1,2,3,4,5,6,7,8,9,10,11\}$, 
\begin{equation}
\label{T_uniform}
    T \sim \text{Uniform}(\{1,2,3,4,5,6,7,8,9,10,11\}).
\end{equation}

Now we have all the notation needed to define the model for how the participants make decisions.



\section{The Model for the Decisions}
Having the notation for the expected losses and decisions, we can define the probability mass function for the decisions using a softmax function similar to the one in \eqref{softmax_in_theory}. 

For each participant we have observed decisions, $\boldsymbol{\delta}=(\delta_1,\delta_2,...,\delta_n)$, where $\delta_j \in \{0,1,2 \}$ as in \eqref{def_of_delta}, and $n$ is the total amount of decisions we have for each participant. Thus, $j\in \{1,2,...,n \}$. As the participants have opened a different amount of boxes each time, $n$ varies form participant to participant. Recall that $i$ is the number of boxes that are opened, and that $i$ is reset for each new trial. Thus, the probability mass function for the decisions can be expressed as
\begin{equation}
\label{softmax_theta}
    f(\delta_{j}|\vp,\eta;x_{1:i}) = \frac{\text{exp}(- \eta \EE^i_{\delta_{j}}(\vp))}{\sum_{d=0}^{2} \text{exp}(-\eta \EE^i_{d}(\vp))},
\end{equation}
where $\eta$ is some parameter. $\eta$ can be interpreted as a measure of how far the choices the participant makes are away from the decision with the least expected loss. If $\eta$ is infinity, they always make the decision with the lowest expected loss, and if $\eta$ is zero, they choose arbitrarily. A negative value of $\eta$ indicates that the participant tends to choose the decisions with higher expected losses.


When we have this model, we can find estimates of the parameters, $\vp$ and $\eta$, for each participant such that the model is adapted to each one of the participants. This estimation is done by finding the maximum likelihood estimates (MLEs) as described in Chapter \ref{section_theory_mle}. 
We can also find confidence intervals tied to each of the parameters for all of the participants using the bootstrap as described in Chapter \ref{section_theory_bootstrap}. This will be done in the subsequent sections, but firstly we find an Ideal Observer solution of the box task and use this to find expressions for the loss functions and expected losses. 




\section{Loss Functions}
Before we start finding Ideal Observer solutions, we formulate loss functions in the unlimited and limited cases. 

\subsection{Loss Functions in the Unlimited Case}
Starting with the unlimited case, we define loss functions for each of the three choices we have when $i$ boxes are opened and put them together as one function. If the participant chooses blue as the majority colour, $\delta_i=0$, we say that the loss is zero if blue is the true majority colour and one if it is not. Thus, this can be expressed as an indicator function as in \eqref{loss_func_indicator}. Recall that the true majority colour is denoted $Z$. Then, we can express the loss of choosing blue as the majority colour when $i$ boxes are opened as
\begin{equation}
\label{loss_func_blue}
    L_i[Z,\delta_i=0;\vp] = I(Z\neq0) = I(Z=1).
\end{equation}

We define the loss function for when the participant chooses that red is the majority colour, $\delta_i=1$, similarly to \eqref{loss_func_blue}. This time the loss is zero if the true majority colour is red and one if the blue is the true majority colour. Thus,
\begin{equation}
\label{loss_func_red}
    L_i[Z,\delta_i=1;\vp] = I(Z\neq1)=I(Z=0).
\end{equation}

We imagine that some participants have some minor penalty or loss of opening another box. That might be because it is tiresome for them to sit through a full trial and they want to finish fast, or that they get some inner reward or feeling of victory when they finish early. A parameter, $\alpha$, represent this. 
The loss function for the choice of opening the next box depends on the successive losses. As we do not know the choices that will be made later, we do not know what these losses are. However, we can model these choices as the choices that an Ideal Observer would make. These choices depend on the colour of the next box, $X_{i+1}$, and the colours of the already opened boxes, $x_{1:i}$. We denote these choices as $IO(x_{1:i},X_{i+1})$, where $X_{i+1} \in \{0,1\}$.
%The loss function for opening the next box depends on the parameter $\alpha$. Recall that this is a small loss, potentially zero, for opening a box. It also depends on the loss after the next box is opened. 
We define the loss for the decision to open the next box as $\alpha$ plus the loss in the next step. Thus, 
\begin{equation}
\label{loss_2_unlim}
    L_i[Z,\delta_i=2;\vp] = \alpha + L_{i+1}[Z,IO(x_{1:i},X_{i+1});\vp],
\end{equation}
where $L_{i+1}[Z,IO(x_{1:i},X_{i+1});\vp]$ is the loss when the next box is opened. 


Putting \eqref{loss_func_blue}, \eqref{loss_func_red} and \eqref{loss_2_unlim} together, we get that the total loss function in the unlimited case can be expressed as
\begin{equation}
\label{tot_loss_unlim}
    \begin{aligned}
       L_i[Z,\delta_{i};\vp] 
       =& I(Z=0)\: I(\delta_i=0) \\
       +& I(Z=1)\: I(\delta_i=1)\\
       +& \big(\alpha + L_{i+1}[Z,IO(x_{1:i},X_{i+1});\vp] \big) I(\delta_i=2).
    \end{aligned}
\end{equation}

Having the loss functions for the unlimited case, we proceed with formulating the loss functions for the limited trials of the box task. 




\subsection{Loss Functions in the Limited Case}
The loss functions in the limited case are highly comparable to the ones in the unlimited case. Recall that in a limited trial, the participants might be stopped when a random box opens and that this counts as a failed trial. 

Firstly, we have a look at the loss function for choosing blue as the majority colour. We see that this is not dependent on any of the boxes that are not opened in the unlimited case. When $i$ boxes are opened in a limited trial, and the participant chooses that blue is the majority colour, this is, as in the unlimited trial, not affected by the colours of the unopened boxes. If $i$ boxes are opened, and one chooses what the majority colour is here, we know that the test will not terminate, as the participant will not open more boxes. Thus, we can put the loss function for choosing blue as the majority colour in a limited trial as the same as the loss function for choosing blue in an unlimited trial. The loss function is then as in \eqref{loss_func_blue}.

The same argument holds for the loss function for choosing red as the majority colour in a limited trial. Thus, that loss function is the same as in \eqref{loss_func_red}.

For the choice of opening the next box, we have to consider that the test might terminate. 
We define a parameter, $\beta$, that only appears in the loss function for opening the next box in limited trials. 
We let it be the loss the participant gets when the test terminates before choosing what the majority colour is.
%Recall that $\beta$ is the loss when the test terminates and that $T$ is the number of boxes that already are opened when the test terminates. 
Recall that $T$ is the number of boxes that already are opened when the test terminates and that it is uniformly distributed as in \eqref{T_uniform}. The loss when the test does not terminate will be the loss for when the next box is open, in the same way as for the unlimited trials. We can include the event of the test terminating as an indicator function, where an indicator function is as seen in \eqref{loss_func_indicator}. Thus, the loss function for opening the next box in an unlimited trial is the loss you get when the next box is opened plus $\alpha$, times an indicator function that is one if the test does not terminate. I addition to this, we have the loss for when the test terminates, $\beta$, times an indicator for that the test terminates. Hence,
\begin{equation}
\label{loss_func_2_limited}
    \begin{aligned}
        L_i[Z,\delta_i=2;\vp] 
        =& \big( \alpha + L_i[Z,IO(x_{1:i},X_{i+1});\vp] \big) \: I(T\neq i) \\
        &+ \beta \: I(T=i),
    \end{aligned}
\end{equation}
where $IO(x_{1:i},X_{i+1})$ are the choices that an Ideal Observer would do in the next steps. 

We get the total loss function in the limited case using \eqref{loss_func_blue}, \eqref{loss_func_red} and \eqref{loss_func_2_limited}, such that
\begin{equation}
\label{total_loss_limited}
    \begin{aligned}
       L_i[Z,\delta_{i};\vp] 
       =& I(Z=0)\: I(\delta_i=0) \\
       +& I(Z=1)\: I(\delta_i=1)\\
       +& \Big(  \big( \alpha + L_i[Z,IO(x_{1:i},X_{i+1});\vp] \big) \: I(T\neq i) \\        
       &+ \beta \: I(T=i)  \Big) I(\delta_i=2)
    \end{aligned}
\end{equation}

As we have the loss functions in the unlimited and the limited cases, we now continue with finding an Ideal Observer solution of the box task. This solution depends on the expectation of the loss functions, the expected losses. 

\section{Ideal Observer Solution}
%\sectionmark{An Ideal Observer Solution}
Having defined loss functions for both the unlimited and the limited versions of the box task, we now want to find an Ideal Observer (IO) solution. We find one for the unlimited case and another for the limited. As stated above, an Ideal Observer acts like a participant that always makes optimal decisions. In the box task, the optimal decision for each opened box is the decision that gives the least expected loss. If one makes the decision with the lowest expected loss each time a box is opened, the total solution is the Ideal Observer solution. Thus, we need to find these expected losses. They depend on the parameter $\alpha$ in the unlimited version of the box task and both $\alpha$ and $\beta$ in the limited version. Therefore, we get many different IO solutions depending on the values of the parameters. 
Recall that when $i$ boxes are opened, the participants have three choices as stated in \eqref{def_of_delta}. We want to find expected losses for all three decisions in both the unlimited and limited versions of the box task. 


 

\subsection{Expected Losses}
\label{section:exp_losses}
When we find the expected losses, we take the expectation of the loss functions like in \eqref{expectation_of_loss_func_general}. Recall that taking the expectation of an indicator function gives the probability of the event and that $x_{1:i}$ is a vector containing the colours of the $i$ opened boxes.

As stated in Chapter \ref{section_notation}, the expected losses when $i$ boxes are opened, are denoted $\EE_{\delta_i}^i(\vp)$, with $\delta_i \in (0,1,2)$.
We start with the expected loss for choosing blue as the majority colour, $\EE^i_0(\vp)$. This expected loss is the same for both the unlimited and limited trials as the loss functions, stated in \eqref{loss_func_blue}, are identical. 
We condition on the colours of the opened boxes, as the true majority colour, $Z$, depends on the colours of all the twelve boxes. Thus,
\begin{equation}
\label{exp_loss_blue}
    \begin{aligned}
        \EE^i_{0}(\vp) = &E\big[ L_i[Z,\delta_i=0;\vp] \big |X_{1:i}=x_{1:i} ]\\
        = & E \big[ I(Z=1) \big|X_{1:i}=x_{1:i}]\\
        = & P(Z=1|X_{1:i}=x_{1:i}).
    \end{aligned}
\end{equation}
We see that the expected loss of choosing blue as the majority colour is equal to the probability that red is the majority colour given the colours of the opened boxes, for both the unlimited and limited versions. The only thing that this expected loss depends on is the colours of the first $i$ boxes, $x_{1:i}$. 
%\begin{equation*}
%    \theta = x_{1:i}.
%\end{equation*}

We find the expected loss of choosing red as the majority colour when $i$ boxes are opened, $\EE^i_{1}(\vp)$, similarly to \eqref{exp_loss_blue}. Again, conditioning on $X_{1:i}=x_{1:i}$, we get
\begin{equation}
\label{exp_loss_red}
    \begin{aligned}
        \EE^i_{1}(\vp) 
        = &E\big[ L_i[Z,\delta_i=1;\vp] \big |X_{1:i}=x_{1:i} ]\\
        = & E \big[ I(Z=0) \big|X_{1:i}=x_{1:i}]\\
        = & P(Z=0|X_{1:i}=x_{1:i}).
    \end{aligned}
\end{equation}
The expected loss for choosing red as the majority colour is then the probability that blue is the majority colour, conditioned on $X_{1:i}=x_{1:i}$, for both the unlimited and the limited case.

When we find the expected losses for opening the next box, we have to distinguish between the unlimited and limited cases. Starting with the unlimited case, we continue in the same way as for choosing blue or red as the majority colour, by taking the expectation of the loss function, as it is stated in \eqref{loss_2_unlim}, and conditioning on the colours of the $i$ opened boxes. Recall that $IO(x_{1:i},X_{i+1})$ are the choices that an Ideal Observer would make in the next steps. We then get that
\begin{equation*}
    \begin{aligned}
        \EE^i_{2}(\vp) 
        =& E \big[ \alpha + L_i[Z,IO(x_{1:i},X_{i+1});\vp] |X_{1:i}=x_{1:i}\big].
    \end{aligned}
\end{equation*}
Taking the expectation of a constant gives the constant. Thus
\begin{equation*}
    E[\alpha|X_{1:i}=x_{1:i}]=\alpha,
\end{equation*}
as $\alpha$ is not dependent on the colours of the boxes. Then,
\begin{equation}
\label{exp_loss_unlim_2_unfinished}
    \begin{aligned}
        \EE^i_{2}(\vp) 
        =& \alpha + E \big[L_i[Z,IO(x_{1:i},X_{i+1});\vp] |X_{1:i}=x_{1:i}\big].
    \end{aligned}
\end{equation}
We see that $E \big[L_i[Z,IO(x_{1:i},X_{i+1});\vp] |X_{1:i}=x_{1:i}\big]$ is the expected loss in the next step, and it depends on the colour of the box that opens, $X_{i+1}$. We find this expectation using the law of total expectation as in \eqref{law_tot_exp_func}. Then,
\begin{equation}
\label{exp_loss_next_unlim}
    \begin{aligned}
        E \big[L_i&[Z,IO(x_{1:i},X_{i+1});\vp] |X_{1:i}=x_{1:i}\big] \\
        =& E \big[L_{i+1}[Z,IO(x_{1:i},X_{i+1});\vp] |X_{1:i}=x_{1:i},X_{i+1}=0\big]\\
        \times& P(X_{i+1}=0|X_{1:i}=x_{1:i}) \\
        +& E \big[L_{i+1}[Z,IO(x_{1:i},X_{i+1});\vp] |X_{1:i}=x_{1:i},X_{i+1}=1\big]\\
        \times & P(X_{i+1}=1|X_{1:i}=x_{1:i}),
    \end{aligned}
\end{equation}
where $P(X_{i+1}=0|X_{1:i}=x_{1:i})$ is the probability that box $i+1$ is blue given the colours of the first $i$ boxes and $P(X_{i+1}=1|X_{1:i}=x_{1:i})$ is the probability that it is red.  

%Eller skal jeg gjÃ¸re det sÃ¥nn her: We see that $E \big[L[Z,\delta_{(i+1):12}] \big|x_{1:i}]$ is the expected loss in the next step, which we denote as $\EE_{\delta_{i+1},i+1}$. 

%Write about those $\delta_{i+1}$ actually being the IO choices. Thus, these are the choices in the future steps that have the lest expected loss. And this is how we find the exp loss in this step, thus, it depends on the IO solution. Call these decisions $IO(x_{1:i},0)$ if box $i+1$ is blue, and $IO(x_{1:i},1)$ if it is red. 

%Thus,
%\begin{equation*}
%    \EE_{2,i} = \alpha + \EE_{\delta_{i+1},i+1}.
%\end{equation*}
%$\EE_{\delta_{i+1},i+1}$ depends on the colour of the box that opens, $X_{i+1}$. Let $\EE_{\delta_{i+1},i+1|X_{i+1}=0}$ be the expected loss in the next step when the next box is blue, and $\EE_{\delta_{i+1},i+1|X_{i+1}=1}$ when box $i+1$ is red. We find the expectation using the law of total expectation as in \eqref{law_tot_exp_func}. Then,
%\begin{equation}
%\label{exp_loss_next_unlim}
%    \begin{aligned}
%        \EE_{\delta_{i+1},i+1}
%        = &\EE_{\delta_{i+1},i+1|X_{i+1}=0} P(X_{i+1}=0|x_{1:i})\\
%        + &\EE_{\delta_{i+1},i+1|X_{i+1}=1} P(X_{i+1}=1|x_{1:i}),
%    \end{aligned}
%\end{equation}
%where $P(X_{i+1}=0|x_{1:i})$ is the probability that box $i+1$ is blue given the colours of the first $i$ boxes and $P(X_{i+1}=1|x_{1:i})$ is the probability that it is red. 


Inserting \eqref{exp_loss_next_unlim} into \eqref{exp_loss_unlim_2_unfinished}, we get that the expected loss for opening the next box in the unlimited case is
\begin{equation}
\label{intermediate_exp_loss_2_unlim}
    \begin{aligned}
        \EE^i_{2}(\vp) = \alpha 
        + &E \big[L_{i+1}[Z,IO(x_{1:i},X_{i+1});\vp] |X_{1:i}=x_{1:i},X_{i+1}=0\big]\\
        \times & P(X_{i+1}=0|X_{1:i}=x_{1:i}) \\
        +& E \big[L_{i+1}[Z,IO(x_{1:i},X_{i+1});\vp] |X_{1:i}=x_{1:i},X_{i+1}=1\big]\\
        \times& P(X_{i+1}=1|X_{1:i}=x_{1:i}).
    \end{aligned}
\end{equation}
Note that
\begin{equation*}
    E \big[L_{i+1}[Z,IO(x_{1:i},X_{i+1});\vp] |X_{1:i}=x_{1:i},X_{i+1}=0\big] = \EE^{i+1}_{IO(x_{1:i},0)}(\vp)
\end{equation*}
and
\begin{equation*}
    E \big[L_{i+1}[Z,IO(x_{1:i},X_{i+1});\vp] |X_{1:i}=x_{1:i},X_{i+1}=1\big] = \EE^{i+1}_{IO(x_{1:i},1)}(\vp).
\end{equation*}
The expression for the expected loss of opening the next box in the unlimited case, \eqref{intermediate_exp_loss_2_unlim}, is then
\begin{equation}
\label{exp_loss_next_box_unlim}
    \begin{aligned}
        \EE^i_{2}(\vp) 
        = \alpha + &\EE^{i+1}_{IO(x_{1:i},0)}(\vp) P(X_{i+1}=0|X_{1:i}=x_{1:i})\\
        + &\EE^{i+1}_{IO(x_{1:i},1)} (\vp)
        P(X_{i+1}=1|X_{1:i}=x_{1:i}).
    \end{aligned}
\end{equation}
In the unlimited case, the expected loss depends on the parameter $\alpha$. Thus, $\vp = \alpha$. 

We proceed in a similar manner when we find the expected loss of opening the next box in the limited case. Taking the expectation of the loss function in \eqref{loss_func_2_limited}, we get the expected loss when $i$ boxes are opened given that the test has not terminated yet, $\EE^i_2(\vp)$. Recall that if the box terminates when $i$ boxes already are open, then the parameter $T$ is equal to $i$. We have to condition on $T$ being greater than or equal to $i$ when we find the expected loss, meaning that the test has not terminated yet when $i$ boxes are open. Using \eqref{loss_func_2_limited} we get that
\begin{equation}
\label{exp_loss_limited_a}
    \begin{aligned}
        \EE^i_{2}(\vp) 
        = &E\big[L_i[Z,\delta_i=2;\vp]\: | \: X_{1:i}=x_{1:i}, T\geq i\big] \\
        =& E\big[\big( \alpha + L_{i+1}[Z,IO(x_{1:i},X_{i+1});\vp] \big) \: I(T\neq i) \\
        &+ \beta \: I(T=i) \: | \: X_{1:i}= x_{1:i}, T\geq i\big].
    \end{aligned}
\end{equation}
We start with the first term in \eqref{exp_loss_limited_a}. When the test terminates is independent of the colours of the boxes, such that $T$ is independent of $x_{1:i}$. The indicator function will then be the probability of $T\neq i$, whereas, for the expectation of the loss function when $i+1$ boxes are opened, we use the law of total expectation as in \eqref{law_tot_exp_func}, and condition on the colour of the next box, $X_{i+1}$. 

We also have that $\EE^{i+1}_{2}(\vp,X_{i+1}=j)$ is the expected loss in the next step given the colours of the $i$ opened boxes,  the colour of box $i+1$ and given that the test has not terminated yet. Thus,
\begin{equation}
\label{exp_loss_limited_b1}
    \begin{aligned}
        E&\big[\big( \alpha + L_{i+1}[Z,IO(x_{1:i},X_{i+1});\vp] \big) \: I(T\neq i) |X_{1:i}= x_{1:i}, T\geq i \big] \\
        =& \Big( \alpha + \sum_{j=0}^1 \EE^{i+1}_{2}(\vp,X_{i+1}=j) P(X_{i+1}=j|X_{1:i}=x_{1:i},T\geq i) \Big)\\
        &\times P(T \neq i|T\geq i)
    \end{aligned}
\end{equation}
%Tregner jeg egentlig Ã¥ ha $T\geq i$ i $P(X_{i+1}=j|x_{1:i},T\geq i)$???
As the $x$'s and $T$ are independent, we have that
\begin{equation}
\label{x_indep_of_T_next_box_colour}
    P(X_{i+1}=j|X_{1:i}=x_{1:i},T\geq i) = P(X_{i+1}=j|X_{1:i}=x_{1:i}).
\end{equation}
Putting \eqref{x_indep_of_T_next_box_colour} into \eqref{exp_loss_limited_b1}, we get
\begin{equation}
\label{exp_loss_limited_b}
    \begin{aligned}
        E&\big[\big( \alpha + L_{i+1}[Z,IO(x_{1:i},X_{i+1});\vp] \big) \: I(T\neq i) |X_{1:i}= x_{1:i}, T\geq i \big] \\
        =& \Big( \alpha + \sum_{j=0}^1 \EE^{i+1}_{2}(\vp,X_{i+1}=j) P(X_{i+1}=j|X_{1:i}=x_{1:i}) \Big)\\
        &\times P(T \neq i|T\geq i).
    \end{aligned}
\end{equation}

The last term in \eqref{exp_loss_limited_a} becomes
\begin{equation}
\label{exp_loss_limited_c}
    \begin{aligned}
        E\big[ \beta \: I(T=i)|X_{1:i}=x_{1:i}, T\geq i\big] 
        = \beta \: P(T=i|T\geq i)
    \end{aligned}
\end{equation}
as it does not depend on the colours of the boxes, $x_{1:i}$.

Putting \eqref{exp_loss_limited_b} and \eqref{exp_loss_limited_c} into \eqref{exp_loss_limited_a}, we get that the expected loss for opening another box in the limited case is
\begin{equation}
\label{exp_loss_limited_final}
    \begin{aligned}
        \EE^i_{2}(\vp) 
        =& \Big( \alpha + \sum_{j=0}^1 \EE^{i+1}_{2}(\vp,X_{i+1}=j) P(X_{i+1}=j|X_{1:i}=x_{1:i}) \Big) \\[6pt]
        &\times 
        P(T \neq i|T\geq i)\\[6pt]
        &+ \beta \: P(T=i|T\geq i).
    \end{aligned}
\end{equation}
The expected losses in the limited cases depend on $\alpha$ and $\beta$, thus in this case we have that $\vp = (\alpha,\beta)$.

Now that we have expressions for the expected losses, we have to find the probabilities in these expressions. 



\subsection{Probabilities}
As we now have expressions for the expected losses, we find the probabilities needed for finding the expected losses. That is $P(Z=1|X_{1:i}=x_{1:i})$, $P(Z=0|X_{1:i}=x_{1:i})$, $P(X_{i+1}=1|X_{1:i}=x_{1:i})$, $P(X_{i+1}=0|X_{1:i}=x_{1:i})$, $P(T\neq i|T\geq i)$ and $P(T=i|T\geq i)$.

\subsubsection{The Majority Colour}
When we find the probabilities used in the expressions for the expected losses, we start with the expected loss for choosing blue as the majority colour, as given in \eqref{exp_loss_blue}. Then we need the probability
\begin{equation}
\label{prob_red_major_Z}
    P(Z=1|X_{1:i}=x_{1:i}).
\end{equation}
%$P(Z=1|x_{1:i})$. 
This is the probability that red is the majority colour, given the colours of the boxes that already are observed. Using the definition of $Z$ as it is in \eqref{def_of_Z_2}, we can express \eqref{prob_red_major_Z} using $U_i$ and $V_i$. Recall that they are defined as in \eqref{def_of_U} and \eqref{def_of_V}, respectively. \eqref{prob_red_major_Z} can then be expressed as the probability that $U_i+V_i>6$, or that $U_i+V_i \geq 7$, given the colours of the $i$ first boxes. However, we also need to condition on $U_i+V_i \neq 6$, as we know that one of the colours always is in majority, such that there will never be six blue and six red boxes all together. Thus, we find $P(U_i+V_i \geq 7 | X_{1:i}=x_{1:i},U_i+V_i \neq 6)$. As the order the boxes have been opened in is irrelevant here, and $U_i = \sum_{j=1}^i X_j$, we use $U_i=u_i$ instead of $X_{1:i}=x_{1:i}$, to be consistent with the other notation. Thus, we have that
\begin{equation}
\label{Z_to_U_and_V}
    P(Z=1|X_{1:i}=x_{1:i}) = P(U_i+V_i \geq 7 | U_i=u_i,U_i+V_i \neq 6).
\end{equation}

Using Bayes rule as described in \eqref{bayesrule}, we get that
\begin{equation}
\label{redmajor2}
    \begin{aligned}
        P(&U_i+V_i \geq 7 | U_i=u_i,U_i+V_i \neq 6) \\[6pt]
        &= \frac{P(U_i+V_i\neq6|U_i=u_i,U_i+V_i\geq7)P(U_i+V_i\geq7|U_i=u_i)}{P(U_i+V_i\neq6|U_i=u_i)}.
    \end{aligned}
\end{equation}
As 
\begin{equation*}
    P(U_i+V_i\neq6|U_i=u_i,U_i+V_i\geq7) = 1,
\end{equation*}
we get that
\begin{equation}
\label{redmajor3}
    \begin{aligned}
        P(U_i+V_i \geq 7 | U_i=u_i,U_i+V_i \neq 6)
        = \frac{P(U_i+V_i\geq7|U_i=u_i)}{P(U_i+V_i\neq6|U_i=u_i)}.
    \end{aligned}
\end{equation}


We have that
\begin{equation}
\label{redmajor1}
    \begin{aligned}
        P(U_i+V_i \geq 7 | U_i=u_i) 
        &= \sum_{j=7}^{12} P(U_i+V_i = j | U_i=u_i)\\[6pt]
    \end{aligned}
\end{equation}
Thus, to be able to find $P(U_i+V_i \geq 7 | U_i=u_i)$, we start with finding $P(U_i+V_i = j | U_i=u_i)$. Using the law of total probability as in \eqref{lawoftotprob}, and conditioning on $\theta$, we get
\begin{equation} 
\label{prob_red_major}
    \begin{aligned}
        P(U_i&+V_i = j | U_i=u_i) \\[6pt]
        =& \int_0^1 P(U_i+V_i = j | U_i=u_i, \Theta=\theta) f(\theta| U_i=u_i) \dd \theta \\[6pt]
        =& \int_0^1 P(V_i = j-u_i | \Theta=\theta) f(\theta| U_i=u_i) \dd \theta
        %\\[6pt]
        %=& \int_0^1 \binom{12-i}{j-u_i} p^{j-u_i}(1-p)^{12-i-(j-u_i)} 
        % \frac{1}{\text{B}(\gamma,\kappa)}p^{\gamma-1}(1-p)^{\kappa-1} dP \\[6pt]
        %=& \frac{1}{\text{B}(\gamma,\kappa)} \binom{12-i}{j-u_i} \int_0^1 p^{j-u_i+\gamma-1} (1-p)^{12-i-(j-u_i)+\kappa-1} dP.
    \end{aligned}
\end{equation}
Thus, we need to find $P(V_i = j-u_i | \Theta=\theta)$ and $f(\theta| U_i=u_i)$. 

Since $V_i$ has a binomial distribution as in \eqref{U_V_binomal_distri}, we get that
\begin{equation}
\label{vi_equal_j_minus_ui}
    P(V_i=j-u_i|\Theta=\theta)=\binom{12-i}{j-u_i}\theta^{j-u_i}(1-\theta)^{12-i-(j-u_i)}
\end{equation}


We can find $f(\theta| U_i=u_i)$ using Bayes rule as given in \eqref{bayesrule}. Hence,
\begin{equation*}
    f(\theta| U_i=u_i) = \frac{P(U_i=u_i|\Theta=\theta)f(\theta)}{P(U_i=u_i)},
\end{equation*}
which is proportional to the numerator of the right-hand side as in \eqref{posterior_proportional}. Using that $U_i|\Theta$ has a binomial distribution with probability mass function as in \eqref{ui_prob_mass}, and that $\Theta$ has a beta prior, with density function as in \eqref{betadistribution}, we get that
\begin{equation*}
    \begin{aligned}
        f(\theta|U_i=u_i) 
        &\propto P(U_i=u_i|\Theta=\theta)f(\theta)\\[6pt] 
        &\propto \theta^{u_i}(1-\theta)^{i-u_i}\theta^{\gamma-1}(1-\theta)^{\kappa-1}\\[6pt]
        &= \theta^{u_i+\gamma-1}(1-\theta)^{i-u_i+\kappa-1}.
    \end{aligned}
\end{equation*}
This is proportional to the density of a beta distribution with parameters $u_i+\gamma$ and $i-u_i+\kappa$. Hence, we can conclude that
\begin{equation*}
    \Theta|U_i \sim \text{Beta}(u_i+\gamma,i-u_i+\kappa),
\end{equation*}
and therefore that 
\begin{equation}
\label{theta_given_ui}
    f(\theta|U_i=u_i) = \frac{1}{\text{B}(u_i+\gamma,i-u_i+\kappa)}\theta^{u_i+\gamma-1}(1-\theta)^{i-u_i+\kappa-1}.
\end{equation}

We now have expressions for $P(V_i=j-u_i|\Theta=\theta)$ and $f(\theta|U_i=u_i)$, as given in \eqref{vi_equal_j_minus_ui} and \eqref{theta_given_ui}, respectively. We put these into \eqref{prob_red_major}, and get
\begin{equation}
\label{red_12_equal_j_a}
    \begin{aligned}
         P(&U_i+V_i = j | U_i=u_i) \\[6pt]
        =& \int_0^1 P(V_i = j-u_i | \Theta=\theta) P(\Theta=\theta| U_i=u_i) \dd \theta \\[6pt]
        =& \int_0^1 \binom{12-i}{j-u_i}\theta^{j-u_i}(1-\theta)^{12-i-(j-u_i)} \frac{\theta^{u_i+\gamma-1}(1-\theta)^{i-u_i+\kappa-1}}{\text{B}(u_i+\gamma,i-u_i+\kappa)} \dd \theta.\\[6pt]
    \end{aligned}
\end{equation}
Taking the parts that do not depend on $\theta$ outside of the integral and summing the exponents of $\theta$ and $(1-\theta)$, we get that \eqref{red_12_equal_j_a} is
\begin{equation}
\label{red_12_equal_j}
    \begin{aligned}
         P(&U_i+V_i = j | U_i=u_i) \\[6pt]
        =& \frac{\binom{12-i}{j-u_i}}{\text{B}(u_i+\gamma,i-u_i+\kappa)} \\[6pt]
        &\times \int_0^1 
        \theta^{j-u_i+u_i+\gamma-1}(1-\theta)^{12-i-(j-u_i)+i-u_i+\kappa-1} \dd \theta\\[6pt]
        =& \frac{\binom{12-i}{j-u_i}}{\text{B}(u_i+\gamma,i-u_i+\kappa)} \int_0^1 
        \theta^{j+\gamma-1}(1-\theta)^{12-j+\kappa-1} \dd \theta.
    \end{aligned}
\end{equation}
The part inside the integral is proportional to the density of a beta distribution with parameters $j+\gamma$ and $12-j+\kappa$. The integral of a density over the parameter space is one, hence
\begin{equation*}
    \int_0^1 \frac{1}{\text{B}(j+\gamma,12-j+\kappa)}\theta^{j+\gamma-1}(1-\theta)^{12-j+\kappa} \dd \theta = 1.
\end{equation*}
Therefore,
\begin{equation}
\label{beta_distr_equal_one}
    \int_0^1 \theta^{j+\gamma-1}(1-\theta)^{12-j+\kappa} \dd \theta = \text{B}(j+\gamma,12-j+\kappa).
\end{equation}
Putting \eqref{beta_distr_equal_one} into \eqref{red_12_equal_j}, we get
\begin{equation}
\label{red_12_equal_j_final}
    \begin{aligned}
        P(U_i+&V_i = j | U_i=u_i) = \binom{12-i}{j-u_i} \frac{\text{B}(j+\gamma,12-j+\kappa)}{\text{B}(u_i+\gamma,i-u_i+\kappa)}.
    \end{aligned}
\end{equation}

Putting \eqref{red_12_equal_j_final} into \eqref{redmajor1}, we get that
\begin{equation}
\label{redmajor_given_u}
%\label{redmajor2}
    \begin{aligned}
        P(U_i+V_i \geq 7 | U_i=u_i) 
        &= \sum_{j=7}^{12} \binom{12-i}{j-u_i} \frac{\text{B}(j+\gamma,12-j+\kappa)}{\text{B}(u_i+\gamma,i-u_i+\kappa)}.
    \end{aligned}
\end{equation}

We have that
\begin{equation*}
    P(U_i+V_i\neq6|U_i=u_i) 
        = 1-P(U_i+V_i=6|U_i=u_i)
\end{equation*}
and, using \eqref{red_12_equal_j_final}, we get 
\begin{equation}
\label{u12neq6}
    \begin{aligned}
        P(U_i+V_i\neq6|U_i=u_i) 
        &= 1-\binom{12-i}{6-u_i} \frac{\text{B}(6+\gamma,12-6+\kappa)}{\text{B}(u_i+\gamma,i-u_i+\kappa)}\\[6pt]
        &= 1-\binom{12-i}{6-u_i} \frac{\text{B}(6+\gamma,6+\kappa)}{\text{B}(u_i+\gamma,i-u_i+\kappa)}.
    \end{aligned}
\end{equation}
Putting \eqref{u12neq6} and \eqref{redmajor_given_u} into \eqref{redmajor3}, we get
\begin{equation}
\label{redmajor_final}
    \begin{aligned}
        P(U_i+V_i \geq 7 | U_i=u_i,U_i+V_i \neq 6)& \\[6pt]
        = \frac{\sum_{j=7}^{12} \binom{12-i}{j-u_i} \frac{\text{B}(j+\gamma,12-j+\kappa)}{\text{B}(u_i+\gamma,i-u_i+\kappa)}}{1-\binom{12-i}{6-u_i} \frac{\text{B}(6+\gamma,6+\kappa)}{\text{B}(u_i+\gamma,i-u_i+\kappa)}}&.
    \end{aligned}
\end{equation}
This is the probability that there is a red majority in total, given the colour of the first $i$ boxes that are opened, and given that one of the colours is in the majority. This is also the expected loss of choosing blue as the majority colour. 


In the expected loss for choosing red as the majority colour, we have $P(Z=0|X_{1:i}=x_{1:i})$, as in \eqref{exp_loss_red}. The same argument holds here as in \eqref{Z_to_U_and_V}. Thus, we have that
\begin{equation}
    P(Z=0|X_{1:i}=x_{1:i}) = P(U_i+V_i \leq 5 | U_i=u_i,U_i+V_i \neq 6).
\end{equation}
This is the probability that blue is the majority colour, which is the complementing probability to the probability that red is the majority colour. Therefore,
\begin{equation}
\label{blue_major}
    \begin{aligned}
        P(U_i+V_i& \leq 5 | U_i=u_i,U_i+V_i \neq 6) \\
        &= 1 - P(U_i+V_i \geq 7 | U_i=u_i,U_i+V_i \neq 6).
    \end{aligned}
\end{equation}
Putting the expression in \eqref{redmajor_final} into \eqref{blue_major}, we get that the probability of blue being the dominant colour is
\begin{equation}
\label{blue_major_final}
    \begin{aligned}
        P(U_i+V_i \leq 5 | U_i=u_i,U_i+V_i \neq 6) &\\[6pt]
        = 1 - \frac{\sum_{j=7}^{12} \binom{12-i}{j-u_i} \frac{\text{B}(j+\gamma,12-j+\kappa)}{\text{B}(u_i+\gamma,i-u_i+\kappa)}}{1-\binom{12-i}{6-u_i} \frac{\text{B}(6+\gamma,6+\kappa)}{\text{B}(u_i+\gamma,i-u_i+\kappa)}}&,
    \end{aligned}
\end{equation}
which also it the expected loss of choosing red as the majority colour. 

\subsubsection{The Colour of the Next Box}
We now have a look at the expected losses for opening the next box, both in the unlimited and the limited cases, as given in \eqref{exp_loss_next_box_unlim} and \eqref{exp_loss_limited_final}, respectively. In both expressions, we have the probability that the next box is either red or blue, given the colours of the opened boxes. 
These probabilities are $P(X_{i+1}=1|X_{1:i}=x_{1:i})$ and $P(X_{i+1}=0|X_{1:i}=x_{1:i})$, where 
\begin{equation}
\label{next_blue_from_red_unlim}
    P(X_{i+1}=0|X_{1:i}=x_{1:i}) = 1 - P(X_{i+1}=1|X_{1:i}=x_{1:i}),
\end{equation}
as there are only two possible colours the box could have. Thus, we find the probability that that the next box is red, and can then easily find the probability of it being blue using \eqref{next_blue_from_red_unlim}.

Again, we change the notation from $X_{1:i}=x_{1:i}$ to $U_i=u_i$ and $V_i=v_i$, with the same argument as for \eqref{Z_to_U_and_V}. Thus,
\begin{equation}
    \begin{aligned}
        P(X_{i+1}=1|X_{1:i}=x_{1:i}) = P(X_{i+1}=1|U_i=u_i,U_i+V_i\neq6)
    \end{aligned}
\end{equation}
Using Bayes' rule, stated in \eqref{bayesrule}, we get that this is
\begin{equation}
\label{nextisred_bayes_rule}
    \begin{aligned}
        P(&X_{i+1}=1|U_i=u_i,U_i+V_i\neq6) \\[6pt]
        &= \frac{P(U_i+V_i\neq6|U_i=u_i,X_{i+1}=1)P(X_{i+1}=1|U_i=u_i)}
        {P(U_i+V_i\neq6|U_i=u_i)},
    \end{aligned}
\end{equation}
where the expression in the denominator, $P(U_i+V_i\neq6|U_i=u_i)$, is as given in \eqref{u12neq6}. 

We start by finding $P(X_{i+1}=1|U_i=u_i)$. Using the law of total probability that is stated in \eqref{lawoftotprob}, and conditioning on $\theta$, we get
\begin{equation}
\label{xiplus1_given_ui1}
    \begin{aligned}
        P(X_{i+1}=1|U_i=u_i)
        = \int_0^1 & P(X_{i+1}=1|U_i=u_i,\Theta=\theta)\\[6pt]
        &\times f(\theta|U_i=u_i)\: \dd \theta.
    \end{aligned}
\end{equation}
The expression for $f(\theta|U_i=u_i)$ is given in \eqref{theta_given_ui}. All of the $x$'s are Bernoulli distributed with probability $\theta$, and they are conditionally independent of each other, given $\theta$. Therefore, the probability that $X_{i+1}$ is one, or red, is independent of the colour of the already opened boxes. The probability that a box that is opened is one is also equal to $\theta$. Hence,
\begin{equation}
\label{nextisred_equal_theta}
    \begin{aligned}
        P(X_{i+1}=1|U_i=u_i,\Theta=\theta) = P(X_{i+1}=1|\Theta=\theta) = \theta.
    \end{aligned}
\end{equation}
Putting \eqref{nextisred_equal_theta} and \eqref{theta_given_ui} into \eqref{xiplus1_given_ui1} gives
\begin{equation}
\label{xiplus1_given_ui2}
    \begin{aligned}
        P(&X_{i+1}=1|U_i=u_i)\\
        &= \int_0^1 \theta \frac{1}{\text{B}(u_i+\gamma,i-u_i+\kappa)}\theta^{u_i+\gamma-1}(1-\theta)^{i-u_i+\kappa-1}  \dd \theta\\[6pt]
        &=\frac{1}{\text{B}(u_i+\gamma,i-u_i+\kappa)} \int_0^1 \theta^{u_i+\gamma}(1-\theta)^{i-u_i+\kappa-1} \dd \theta.
    \end{aligned}
\end{equation}
Again, the part inside the integral is proportional to the density of a beta distribution, here with parameters $u_i+\gamma+1$ and $i-u_i+\kappa$. Integrating a distribution over the parameter space gives one, which in this case gives
\begin{equation*}
    \begin{aligned}
        \int_0^1 \frac{1}{\text{B}(u_i+\gamma+1,i-u_i+\kappa)} \theta^{u_i+\gamma}(1-\theta)^{i-u_i+\kappa-1}  \dd \theta = 1.
    \end{aligned}
\end{equation*}
Hence,
\begin{equation}
\label{integral_of_beta_distr}
    \begin{aligned}
        \int_0^1 \theta^{u_i+\gamma}(1-\theta)^{i-u_i+\kappa-1} \dd \theta = \text{B}(u_i+\gamma+1,i-u_i+\kappa).
    \end{aligned}
\end{equation}
Inserting \eqref{integral_of_beta_distr} into \eqref{xiplus1_given_ui2} gives
\begin{equation}
\label{xiplus1_given_ui3}
    \begin{aligned}
        P(X_{i+1}=1|U_i=u_i) 
        = \frac{\text{B}(u_i+\gamma+1,i-u_i+\kappa)}
        {\text{B}(u_i+\gamma,i-u_i+\kappa)}.
    \end{aligned}
\end{equation}
Using the property of the beta function as stated in \eqref{beta_as_gamma}, we get that the numerator in \eqref{xiplus1_given_ui3} is
\begin{equation}
\label{beta_to_gamma1}
     \text{B}(u_i+\gamma+1,i-u_i+\kappa)
     =\frac{\Gamma(u_i+\gamma+1)\Gamma(i-u_i+\kappa)}{\Gamma(u_i+\gamma+1+i-u_i+\kappa)},
\end{equation}
and that the denominator is
\begin{equation}
\label{beta_to_gamma2}
    \text{B}(u_i+\gamma,i-u_i+\kappa)
    =\frac{\Gamma(u_i+\gamma)\Gamma(i-u_i+\kappa)}{\Gamma(u_i+\gamma+i-u_i+\kappa)}.
\end{equation}
Inserting \eqref{beta_to_gamma1} and \eqref{beta_to_gamma2} into \eqref{xiplus1_given_ui3}, gives
\begin{equation}
\label{xiplus1_given_ui4}
    \begin{aligned}
        P(X_{i+1}=1|U_i=u_i) 
        &= \frac{\frac{\Gamma(u_i+\gamma+1)\Gamma(i-u_i+\kappa)}{\Gamma(u_i+\gamma+1+i-u_i+\kappa)}}
        {\frac{\Gamma(u_i+\gamma)\Gamma(i-u_i+\kappa)}{\Gamma(u_i+\gamma+i-u_i+\kappa)}} \\[6pt]
        &= \frac{\frac{\Gamma(u_i+\gamma+1)}{\Gamma(\gamma+1+i+\kappa)}}
        {\frac{\Gamma(u_i+\gamma)}{\Gamma(\gamma+i+\kappa)}}.
    \end{aligned}
\end{equation}
Using the recursive property of the gamma function as seen in \eqref{gamma_recursive_property}, we get that the nominator in \eqref{xiplus1_given_ui4} is
\begin{equation}
\label{recursive_gamma}
    \frac{\Gamma(u_i+\gamma+1)}{\Gamma(\gamma+1+i+\kappa)}
    =\frac{(\gamma+u_i)\Gamma(u_i+\gamma)}{(\gamma+\kappa+i)\Gamma(\gamma+i+\kappa)}.
\end{equation}
Inserting \eqref{recursive_gamma} into \eqref{xiplus1_given_ui4}, we get
\begin{equation}
\label{xiplus1_given_ui5}
    \begin{aligned}
        P(X_{i+1}=1|U_i=u_i) 
        &= \frac{\frac{(\gamma+u_i)\Gamma(u_i+\gamma)}{(\gamma+\kappa+i)\Gamma(\gamma+i+\kappa)}}
        {\frac{\Gamma(u_i+\gamma)}{\Gamma(\gamma+i+\kappa)}}\\[6pt]
        &=\frac{\gamma+u_i}
        {\gamma+\kappa+i}.
    \end{aligned}
\end{equation}

In the expression in \eqref{nextisred_bayes_rule}, it remains to find $P(U_i+V_i\neq6|U_i=u_i,X_{i+1}=1)$.
Firstly, 
\begin{equation}
\label{u_iplus1_equal_ui+1}
    \begin{aligned}
        P&(U_i+V_i\neq6|U_i=u_i,X_{i+1}=1) \\[6pt]
        &= P(U_i+V_i\neq6|U_{i+1}=u_i+1)
        \\[6pt]
        &= P(U_{i+1}+V_{i+1}\neq6|U_{i+1}=u_i+1).
    \end{aligned}
\end{equation}
%From \eqref{u12neq6} we have that
%\begin{equation}
%\label{u12neq6_b}
%    \begin{aligned}
%        P(U_i+V_i\neq6|U_i=u_i) 
%        &= 1-\binom{12-i}{6-u_i} \frac{\text{B}(6+\gamma,6+\kappa)}{\text{B}(u_i+\gamma,i-u_i+\kappa)},
%    \end{aligned}
%\end{equation}
%thus, 
Using \eqref{u12neq6}, we get that
\begin{equation}
\label{u12neq6_c}
    \begin{aligned}
        P&(U_{i+1}+V_{i+1}\neq6|U_{i+1}=u_i+1) \\[6pt]
        &= 1-\binom{12-(i+1)}{6-(u_i+1)} \frac{\text{B}(6+\gamma,6+\kappa)}{\text{B}(u_i+1+\gamma,i-(u_i+1)+\kappa)}\\[6pt]
        &= 1-\binom{11-i}{5-u_i} \frac{\text{B}(6+\gamma,6+\kappa)}{\text{B}(u_i+1+\gamma,i-u_i-1+\kappa)}.
    \end{aligned}
\end{equation}
Then, using \eqref{u_iplus1_equal_ui+1} and \eqref{u12neq6_c}, we get that 
\begin{equation}
\label{u12neq6_givennextisred}
    \begin{aligned}
        P&(U_i+V_i\neq6|U_i=u_i,X_{i+1}=1)\\[6pt] 
        &= 1-\binom{11-i}{5-u_i} \frac{\text{B}(6+\gamma,6+\kappa)}{\text{B}(u_i+\gamma+1,i-u_i+\kappa)}
    \end{aligned}
\end{equation}

Inserting \eqref{u12neq6_givennextisred}, \eqref{xiplus1_given_ui5} and \eqref{u12neq6} into \eqref{nextisred_bayes_rule}, we get
\begin{equation}
\label{nextisred_given_majority}
    \begin{aligned}
        P(&X_{i+1}=1|U_i=u_i,U_i+V_i\neq6) \\[6pt]
        &= \frac{P(U_i+V_i\neq6|U_i=u_i,X_{i+1}=1)P(X_{i+1}=1|U_i=u_i)}
        {P(U_i+V_i\neq6|U_i=u_i)}\\[6pt]
        &= \frac{\bigg[ 1 - \binom{11-i}{5-u_i}\frac{\text{B}(6+\gamma,6+\kappa)}{\text{B}(\gamma+u_i+1,\kappa+i-u_i)} \bigg]
        \frac{\gamma+u_i}
        {\gamma+\kappa+i}}
        {1-\binom{12-i}{6-u_i} \frac{\text{B}(6+\gamma,6+\kappa)}{\text{B}(u_i+\gamma,i-u_i+\kappa)}}.
    \end{aligned}
\end{equation}

As we now have the probability that the next box that is opened is red, we can find the probability that the next box that opens is blue using \eqref{next_blue_from_red_unlim}. Thus, we have both the probabilities for which colour the next box is. 



\subsubsection{When the Test Terminates}
In \eqref{exp_loss_limited_final} we see the expected loss for opening another box in the limited case. Here we have the probability that the test terminates when $i$ boxes are opened given that the test has not terminated yet and the probability that it does not terminate when $i$ boxes are opened given that it has not terminated yet. 
These are $P(T=i|T\geq i)$ and $P(T\neq i|T\geq i)$, which are complementary probabilities, such that
\begin{equation}
\label{T_neq_i}
    P(T \neq i|T\geq i) = 1-P(T=i|T\geq i).
\end{equation}
Thus, if we find $P(T=i|T\geq i)$, we can easily find $P(T\neq i|T\geq i)$ using \eqref{T_neq_i}. 

Using Bayes' rule as it is given in \eqref{bayesrule}, we get
\begin{equation}
\label{T_equal_i_a}
    \begin{aligned}
        P(T=i|T\geq i) = \frac{P(T\geq i|T=i)P(T=i)}{P(T\geq i)}.
    \end{aligned}
\end{equation}
We see that 
\begin{equation}
\label{T_geq_i_given_i}
    P(T\geq i|T=i) = 1.
\end{equation}
As $T$ is uniformly distributed with 11 possible values, as in \eqref{T_uniform}, we have that
\begin{equation}
\label{prob_T_equal_i}
    P(T=i) = \frac{1}{11}.
\end{equation}
It then remains to find $P(T\geq i)$. We have that
\begin{equation}
\label{T_geq_i_unfinished}
    \begin{aligned}
        P(T\geq i) = 1 - P(T<i) = 1 - \sum_{j=1}^{i-1}P(T=j).
    \end{aligned}
\end{equation}
As in \eqref{prob_T_equal_i}, we have that $P(T=j)$ is $\frac{1}{11}$. Thus, \eqref{T_geq_i_unfinished} becomes
\begin{equation}
\label{T_geq_i}
    \begin{aligned}
        P(T\geq i) =& 1-\sum_{j=1}^{i-1} \frac{1}{11} = 1- (i-1)\frac{1}{11}\\[6pt]
        =& \frac{11-(i-1)}{1} = \frac{12-i}{11}.
    \end{aligned}
\end{equation}
Inserting \eqref{T_geq_i_given_i}, \eqref{prob_T_equal_i} and \eqref{T_geq_i} into \eqref{T_equal_i_a}, we get 
\begin{equation}
    \begin{aligned}
        P(T=i|T\geq i) = \frac{\frac{1}{11}}{\frac{12-i}{11}} = \frac{1}{12-i}.
    \end{aligned}
\end{equation}

We now have all that we need to find the three expected losses each time a box is opened in both unlimited and limited trials. Then we find the Ideal Observer solutions by always choosing the decision with the least expected loss each time a box is opened. 



\section{Maximum Likelihood Estimators}
\label{section:mles}
As we have defined a model for the participants' decisions and found expressions for the expected losses for each of the three decisions, we can now fit the model to each participant. We do this by finding maximum likelihood estimates of $\alpha$ and $\eta$ in the unlimited case and $\alpha$, $\beta$ and $\eta$ in the limited case, based on the decisions the participants have made. We generalise the situation to fit both the unlimited and limited case and denote $\alpha$ and $\beta$ as $\vp$. 

We can find the likelihood, $L(\vp,\eta|\boldsymbol{\delta})$ as in \eqref{likelihood}. If we have $n$ decisions for each participant, denoted $\boldsymbol{\delta}$, we get that the likelihood is 
\begin{equation}
\label{likelihood_unfinished}
    \begin{aligned}
       L(\vp,\eta|\boldsymbol{\delta}) = \prod_{j=1}^n f(\delta_j|\vp,\eta)
    \end{aligned}
\end{equation}

Using the model as it is defined in  \eqref{softmax_theta} and the expected losses as they are formulated in Chapter \ref{section:exp_losses}, \eqref{likelihood_unfinished} becomes
\begin{equation}
\label{likelihood_softmax}
    L(\vp,\eta|\boldsymbol{\delta}) = \prod_{j=1}^n \frac{\text{exp}(- \eta \EE^i_{\delta_{j}}(\vp))}{\sum_{d=0}^{2} \text{exp}(-\eta \EE^i_{d}(\vp))}.
\end{equation}
We then find the log likelihood function, $l(\vp,\eta|\boldsymbol{\delta})$, by taking the logarithm of \eqref{likelihood_softmax} as in \eqref{chap2:log_likelihood} and \eqref{chap2:log_likelihood2}. Then,
\begin{equation}
\label{log_likel_softmax_1}
    \begin{aligned}
       l(\vp,\eta|\boldsymbol{\delta})
       =& \sum_{j=1}^n \text{log} (f(\delta_j|\vp,\eta)) \\
       =& \sum_{j=1}^n \text{log} \bigg( \frac{\text{exp}(- \eta \EE^i_{\delta_{j}}(\vp))}{\sum_{d=0}^{2} \text{exp}(-\eta \EE^i_{d}(\vp))} \bigg).
    \end{aligned}
\end{equation}
Using the property of the logarithm that 
\begin{equation*}
    \text{log}\Big(\frac{a}{b} \Big) = \text{log}(a) - \text{log}(b),
\end{equation*}
we get that the log likelihood is
\begin{equation}
\label{log_likel_softmax_2}
    \begin{aligned}
       l(\vp,\eta|\boldsymbol{\delta})
       = \sum_{j=1}^n \bigg( &\text{log} \big( \text{exp}(- \eta \EE^i_{\delta_{j}}(\vp))\big)\\
       - &\text{log} \big( \sum_{d=0}^{2} \text{exp}(-\eta \EE^i_{d}(\vp)) \big)
       \bigg).
    \end{aligned}
\end{equation}
Using that the last term inside the sum does not depend on $j$, and that 
\begin{equation*}
    \text{log} \big( \text{exp}(- \eta \EE^i_{\delta_{j}}(\vp))\big)
    = - \eta \EE^i_{\delta_{j}}(\vp),
\end{equation*}
we can write \eqref{log_likel_softmax_2} as
\begin{equation}
\label{log_likelihood_softmax}
    \begin{aligned}
       l(\vp,\eta|\boldsymbol{\delta})
       = \sum_{j=1}^n \Big( - \eta \EE^i_{\delta_{j}}(\vp) \Big)
       - n \: \text{log} \big( \sum_{d=0}^{2} \text{exp}(-\eta \EE^i_{d}(\vp)) \big).
    \end{aligned}
\end{equation}

It does not matter if we maximise the likelihood or the log-likelihood because maximising \eqref{likelihood_softmax} and \eqref{log_likelihood_softmax} with respect to $\vp$ and $\eta$, gives the same estimates for the parameters. These are the maximum likelihood estimates, and we denote them as $\hat{\vp}$ and $\hat{\eta}$, respectively. We maximize \eqref{log_likelihood_softmax} here. Thus, we find
\begin{equation}
    \hat{\vp},\hat{\eta} = \underset{\vp,\eta}{\mathrm{arg}\,\mathrm{max}}\:\: l(\vp,\eta|\boldsymbol{\delta}) .
\end{equation}

Recall that in the unlimited trials we have $\vp=\alpha$, such that we find $\hat{\alpha}$ and $\hat{\eta}$ for all participants. In the limited trials we have that $\vp=\alpha,\beta$, meaning that we find $\hat{\alpha}$, $\hat{\beta}$ and $\hat{\eta}$ for all participants.

To maximise the log-likelihood, we use a version of the Broydenâ€“Fletcherâ€“ Goldfarbâ€“Shanno (BFGS) algorithm called L-BFGS-B. This algorithm uses less memory than the original BFGS and allows bounds on the parameters \citep{optimization2}. It is an extension of the limited memory BFGS (L-BFGS) algorithm that does not allow bounds on the parameters. L-BFGS-B is mostly used for nonlinear optimisation problems with bounded variables where it might be hard to find the Hessian matrix. 
As we want to maximise \eqref{log_likelihood_softmax} and this depends on the expected losses, it is hard to find the Hessian matrix. Especially the expected loss for opening the next box, $\EE_2^i(\vp)$, makes this challenging as it depends on the expected losses in the next steps. We also have bounds on $\alpha$ and $\beta$. Recall that they are defined as the loss one gets when a new box is opened, and the loss one gets when the test terminates, respectively. If $\alpha$ or $\beta$ were negative, this indicates some reward of opening the next box or the test terminating, which we believe is not likely. Therefore, the L-BFGS-B algorithm is a natural choice of optimisation algorithm. The L-BFGS-B is used for minimising and not maximising. Thus we minimise the negative of the log-likelihood instead of maximising the log-likelihood. We use Python to do this. To avoid finding local minimum points, we use several starting values and choose the results tied to the lowest value of the negative log-likelihood. 

When we have the MLEs for $\alpha$, $\beta$ and $\eta$, it remains to find confidence intervals for the parameters. 


\section{Confidence Intervals for the Parameters}
\label{chapter:CIs_chap3}
To say something about the uncertainty of the parameter estimates we have found, we now find confidence intervals for each of them. We then use parametric bootstrapping, which is described in Chapter \ref{section_theory_bootstrap}.

Consider a person with parameter estimates $\hat{\alpha}$, $\hat{\beta}$ and $\hat{\eta}$. Recall that $\hat{\vp}=\hat{\alpha}$ in the unlimited case and $\hat{\vp}=\hat{\alpha},\hat{\beta}$ in the limited case. 
Using the softmax model, we can find the probabilities for each of the three choices in all the steps in the nine trials. That is, we find the probability that the participant chooses blue as the majority colour, that she chooses red and that she chooses to open the next box. These probabilities are then
\begin{equation}
\label{probabilities_bootstrap}
    P(\delta_{j}|\hat{\vp},\hat{\eta},x_{1:i}) = \frac{\text{exp}(- \hat{\eta} \EE^i_{\delta_j}(\hat{\vp}))}{\sum_{d=0}^{2} \text{exp}(-\hat{\eta} \EE^i_{d}(\hat{\vp}))},
\end{equation}
where $\delta_j \in \{0,1,2 \}$.
For each opened box, we can simulate the decisions the participant would make using these probabilities. Then we get whole new sequences of decisions. Consider, for example, Trial 2, where the order of the boxes is as shown in Figure \ref{fig:trial2_order}. Before any boxes are opened, we can imagine that the probability that the participant chooses to open the next box, that $\delta_0=2$, is relatively high, and the other two probabilities quite small and of equal size. The first box that opens is a red one, $X_1=1$. The probability that she chooses red as the majority colour is higher than the probability that she chooses blue, but whether it is higher than the probability of $\delta_1=2$ or not depends on the parameter estimates. We find these probabilities and draw decisions until the participant in the simulated trial has chosen what the majority colour is. We do that for each of the nine trials and then end up with a new set of simulated decisions. In the limited trials, we either stop when $\delta_i=0$ or $\delta_i=1$, or when the test terminates. If the test terminates, this is a failed trial, and the loss is $\beta$. 

For those simulated decisions, we find new estimates for the parameters, again using maximum likelihood estimation as in Chapter \ref{section:mles}. We simulate these decisions and find the new MLEs 1000 times. We then have 1000 bootstrap samples for all the parameters for each participant. These MLEs of the decisions in simulated trials are denoted
\begin{equation*}
    \{\vp^{*(b)}, b=1,2,...,1000 \}
\end{equation*}
and 
\begin{equation*}
    \{\eta^{*(b)}, b=1,2,...,1000 \}.
\end{equation*}
For each of these parameters, we use the percentile method to construct confidence intervals as described in Chapter \ref{theory_ci_bootstrap}. We find 90\% CIs for each of the parameters, and we denote them as
\begin{equation*}
    [\hat{\vp}^{*(5)}_{1000},\hat{\vp}^{*(95)}_{1000}]
\end{equation*}
and
\begin{equation*}
    [\hat{\eta}^{*(5)}_{1000},\hat{\eta}^{*(95)}_{1000}].
\end{equation*}
Thus, we are finding the 5-th and 95-th percentiles. 

We do this for each participant and get confidence intervals for $\alpha$ and $\eta$ in the unlimited case and $\alpha$, $\beta$ and $\eta$ in the limited case for all 76 participants. 

We now have an Ideal Observer solution of the box task and parameter estimates and confidence intervals for all three parameters for all of the 76 participants. Next, we will show some results.